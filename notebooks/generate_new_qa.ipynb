{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15a093a1-1c82-4493-9421-c3830d70e824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-dotenv in /home/analyst/.local/lib/python3.10/site-packages (1.1.0)\n",
      "✅  Key loaded: True\n",
      "✅  OpenAI client ready!\n"
     ]
    }
   ],
   "source": [
    "# Install the dotenv helper in *this* kernel environment\n",
    "import sys, subprocess\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"python-dotenv\"])\n",
    "\n",
    "# Verify import and load the .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()                               # looks for .env in notebook folder\n",
    "\n",
    "import os\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(\"✅  Key loaded:\", bool(OPENAI_API_KEY))\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "print(\"✅  OpenAI client ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5c9f046-63bc-45f2-8d69-629fc35e9edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()                       # loads .env\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ce55a20-6692-4647-8240-efa1995d9418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  OpenAI version: 1.75.0\n"
     ]
    }
   ],
   "source": [
    "import openai, pkg_resources\n",
    "print(\"✅  OpenAI version:\", pkg_resources.get_distribution('openai').version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a37cc571-a97f-4420-8ec6-9bdb3966e01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY=\"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6504a79d-f921-40a2-8f49-f28de5122540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the Central Limit Theorem and how can it be applied to derive the distribution of the sample mean?…\n",
      "\n",
      "What is the maximum likelihood estimator (MLE) for the parameter \\(\\theta\\) of a normal distribution with known variance…\n",
      "\n",
      "How do you derive the Black-Scholes formula for European call options, and what assumptions underlie its application?…\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json, itertools, pathlib\n",
    "path = pathlib.Path(\"/workspace/data/embeddings/questions/qa_bank_master_v2.jsonl\")\n",
    "for line in itertools.islice(path.open(), 3):\n",
    "    print(json.loads(line)['question'][:120] + \"…\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1375fd5a-80e8-4272-8b9b-0d7a2e475462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged set size: 183 lines → /workspace/data/embeddings/questions/qa_bank_master_merged.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json, hashlib, pathlib, itertools\n",
    "\n",
    "old_path = pathlib.Path(\"/workspace/data/embeddings/questions/qa_bank_master.jsonl\")\n",
    "new_path = pathlib.Path(\"/workspace/data/embeddings/questions/qa_bank_master_v2.jsonl\")\n",
    "out_path = pathlib.Path(\"/workspace/data/embeddings/questions/qa_bank_master_merged.jsonl\")\n",
    "\n",
    "seen = set()\n",
    "with out_path.open(\"w\", encoding=\"utf-8\") as out:\n",
    "    for p in [old_path, new_path]:\n",
    "        with p.open(encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                h = hashlib.sha1(line.encode()).hexdigest()\n",
    "                if h not in seen:\n",
    "                    out.write(line)\n",
    "                    seen.add(h)\n",
    "\n",
    "print(f\"Merged set size: {len(seen)} lines → {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12446aff-371d-480c-8c4a-778e9697e4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  Fine‑tune file ready: /workspace/data/embeddings/questions/qa_bank_master_openai_v2.jsonl\n",
      "Skipped 1 malformed lines (if >0, inspect merge file).\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "merged = Path(\"/workspace/data/embeddings/questions/qa_bank_master_merged.jsonl\")\n",
    "ft_path = merged.with_name(\"qa_bank_master_openai_v2.jsonl\")\n",
    "\n",
    "bad_lines = 0\n",
    "with merged.open() as src, ft_path.open(\"w\", encoding=\"utf-8\") as dst:\n",
    "    for i, line in enumerate(src, 1):\n",
    "        try:\n",
    "            qa = json.loads(line)\n",
    "            if not qa.get(\"question\") or not qa.get(\"answer\"):\n",
    "                raise ValueError(\"Missing fields\")\n",
    "            msg = [\n",
    "                {\"role\": \"user\", \"content\": qa[\"question\"]},\n",
    "                {\"role\": \"assistant\", \"content\": qa[\"answer\"]}\n",
    "            ]\n",
    "            dst.write(json.dumps({\"messages\": msg}, ensure_ascii=False) + \"\\n\")\n",
    "        except Exception:\n",
    "            bad_lines += 1\n",
    "            # optional: log or print(f\"⚠️  Skipping malformed line {i}\")\n",
    "print(f\"✅  Fine‑tune file ready: {ft_path}\")\n",
    "print(f\"Skipped {bad_lines} malformed lines (if >0, inspect merge file).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c26f555a-2233-43a1-8564-7eb1bf7b1e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ total valid rows: 182\n",
      "✓ schema issues:   0  (lines []...)\n",
      "✓ answer length  :  min 21 |  mean 154.2 |  p90 202.6 |  max 300\n"
     ]
    }
   ],
   "source": [
    "import json, pathlib, collections, numpy as np\n",
    "\n",
    "ft_path = pathlib.Path(\"/workspace/data/embeddings/questions/qa_bank_master_openai_v2.jsonl\")\n",
    "\n",
    "n, token_lengths = 0, []\n",
    "bad_schema = []\n",
    "\n",
    "with ft_path.open() as f:\n",
    "    for i, line in enumerate(f, 1):\n",
    "        data = json.loads(line)\n",
    "        if \"messages\" not in data:\n",
    "            bad_schema.append(i); continue\n",
    "        user_msg = next((m for m in data[\"messages\"] if m[\"role\"] == \"user\"), None)\n",
    "        assist_msg = next((m for m in data[\"messages\"] if m[\"role\"] == \"assistant\"), None)\n",
    "        if not user_msg or not assist_msg:\n",
    "            bad_schema.append(i); continue\n",
    "        token_lengths.append(len(assist_msg[\"content\"].split()))\n",
    "        n += 1\n",
    "\n",
    "print(f\"✓ total valid rows: {n}\")\n",
    "print(f\"✓ schema issues:   {len(bad_schema)}  (lines {bad_schema[:5]}...)\")\n",
    "print(\"✓ answer length  :  min {:d} |  mean {:.1f} |  p90 {:.1f} |  max {:d}\".format(\n",
    "      int(np.min(token_lengths)), np.mean(token_lengths),\n",
    "      np.percentile(token_lengths, 90), int(np.max(token_lengths))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d4bd13b-3170-459c-9e26-835021fae798",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ duplicates found: 18\n",
      "  • What is the primary objective in the analysis of limit order books according to …\n",
      "  • How does the focus on execution probabilities rather than descriptive features i…\n",
      "  • In the context of limit order book analysis, explain the relationship between ex…\n",
      "  • What statistical methods would be most appropriate for estimating limit order ex…\n",
      "  • How might the shape of the execution probability curve as a function of limit pr…\n"
     ]
    }
   ],
   "source": [
    "import hashlib, collections, json, pathlib\n",
    "ft_path = pathlib.Path(\"/workspace/data/embeddings/questions/qa_bank_master_openai_v2.jsonl\")\n",
    "\n",
    "hashes = collections.Counter()\n",
    "dupes = []\n",
    "\n",
    "with ft_path.open() as f:\n",
    "    for line in f:\n",
    "        q = json.loads(line)[\"messages\"][0][\"content\"]\n",
    "        h = hashlib.sha1(q.lower().encode()).hexdigest()\n",
    "        hashes[h] += 1\n",
    "        if hashes[h] > 1:\n",
    "            dupes.append(q[:80] + \"…\")\n",
    "\n",
    "print(f\"✓ duplicates found: {len(dupes)}\")\n",
    "if dupes:\n",
    "    for d in dupes[:5]:\n",
    "        print(\"  •\", d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ec9f14d-6421-48e1-a61e-26f92db98008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  Clean file ready: /workspace/data/embeddings/questions/qa_bank_master_openai_v2_clean.jsonl\n",
      "kept 164 | skipped duplicates 18\n"
     ]
    }
   ],
   "source": [
    "import json, hashlib, pathlib\n",
    "\n",
    "src_path   = pathlib.Path(\"/workspace/data/embeddings/questions/qa_bank_master_openai_v2.jsonl\")\n",
    "clean_path = src_path.with_name(\"qa_bank_master_openai_v2_clean.jsonl\")\n",
    "\n",
    "seen = set(); kept = skipped_dup = 0\n",
    "\n",
    "with src_path.open() as src, clean_path.open(\"w\", encoding=\"utf-8\") as dst:\n",
    "    for line in src:\n",
    "        data = json.loads(line)\n",
    "        q = data[\"messages\"][0][\"content\"]\n",
    "        h = hashlib.sha1(q.lower().encode()).hexdigest()\n",
    "        if h in seen:\n",
    "            skipped_dup += 1\n",
    "            continue\n",
    "        dst.write(line); seen.add(h); kept += 1\n",
    "\n",
    "print(f\"✅  Clean file ready: {clean_path}\")\n",
    "print(f\"kept {kept} | skipped duplicates {skipped_dup}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac56c31c-5bbf-419f-9569-beb8bfc8b0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ lines over 4 096‑token limit: 0\n"
     ]
    }
   ],
   "source": [
    "# === Install tiktoken + token‑limit check ====================================\n",
    "import sys, subprocess, json, pathlib\n",
    "\n",
    "# 1. Ensure tiktoken is available in this kernel\n",
    "try:\n",
    "    import tiktoken\n",
    "except ModuleNotFoundError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"tiktoken\"])\n",
    "    import tiktoken\n",
    "\n",
    "# 2. Load clean fine‑tune file\n",
    "ft_path = pathlib.Path(\"/workspace/data/embeddings/questions/qa_bank_master_openai_v2_clean.jsonl\")\n",
    "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "# 3. Count any over‑long rows\n",
    "over = 0\n",
    "with ft_path.open() as f:\n",
    "    for i, line in enumerate(f, 1):\n",
    "        if len(enc.encode(line)) > 4096:\n",
    "            over += 1\n",
    "            print(f\"⚠️  Line {i} exceeds 4 096 tokens\")\n",
    "print(f\"\\n✓ lines over 4 096‑token limit: {over}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2aea8ea9-7856-4fd0-9888-108342519364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ lines over limit: 0\n"
     ]
    }
   ],
   "source": [
    "over = 0\n",
    "with clean_path.open() as f:\n",
    "    for i, line in enumerate(f, 1):\n",
    "        if len(enc.encode(line)) > 4096:\n",
    "            over += 1; print(f\"⚠️  Line {i} > 4096 tokens\")\n",
    "print(f\"✓ lines over limit: {over}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84862689-808e-4599-8c2b-710d2d50c0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: True\n",
      "Size: 209.89 KB\n",
      "First question → Analyze how policy limits in insurance contracts serve as a mechanism to mitigate adverse selection problems, and evalua …\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "clean_path = Path(\"/workspace/data/embeddings/questions/qa_bank_master_openai_v2_clean.jsonl\")\n",
    "\n",
    "print(\"File exists:\", clean_path.exists())\n",
    "if clean_path.exists():\n",
    "    print(\"Size:\", round(clean_path.stat().st_size / 1024, 2), \"KB\")\n",
    "    # peek at first line\n",
    "    with clean_path.open() as f:\n",
    "        first = json.loads(next(f))\n",
    "        print(\"First question →\", first[\"messages\"][0][\"content\"][:120], \"…\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03650e81-a21f-46a1-a4bd-3a23bd9713e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⬆ Uploading file ...\n",
      "✓ File uploaded: file-RnsJm7GDgtnGpa2joZeibG\n",
      "\n",
      "🚀 Launching fine‑tune ...\n",
      "✓ Job ID: ftjob-wer5Sj8ftj6cZtdN3TdIuA2r\n",
      "\n",
      "⏳ Waiting for completion (updates every 20 s) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2%  ETA 00:43"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:37:18 | Created fine-tuning job: ftjob-wer5Sj8ftj6cZtdN3TdIuA2r\n",
      "18:37:18 | Validating training file: file-RnsJm7GDgtnGpa2joZeibG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5%  ETA 27:05"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:38:25 | Files validated, moving job to queued state\n",
      "18:38:26 | Fine-tuning job started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15%  ETA 18:03"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:40:16 | Step 1/492: training loss=0.68\n",
      "18:40:19 | Step 2/492: training loss=0.66\n",
      "18:40:19 | Step 3/492: training loss=2.33\n",
      "18:40:22 | Step 4/492: training loss=0.92\n",
      "18:40:22 | Step 5/492: training loss=2.68\n",
      "18:40:24 | Step 6/492: training loss=2.14\n",
      "18:40:24 | Step 7/492: training loss=0.93\n",
      "18:40:27 | Step 8/492: training loss=1.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35%  ETA 06:34"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:40:27 | Step 9/492: training loss=1.93\n",
      "18:40:30 | Step 10/492: training loss=1.45\n",
      "18:40:30 | Step 11/492: training loss=1.05\n",
      "18:40:33 | Step 12/492: training loss=1.50\n",
      "18:40:33 | Step 13/492: training loss=1.12\n",
      "18:40:36 | Step 14/492: training loss=1.00\n",
      "18:40:36 | Step 15/492: training loss=0.96\n",
      "18:40:38 | Step 16/492: training loss=2.25\n",
      "18:40:39 | Step 17/492: training loss=2.10\n",
      "18:40:41 | Step 18/492: training loss=0.60\n",
      "18:40:41 | Step 19/492: training loss=1.66\n",
      "18:40:44 | Step 20/492: training loss=1.38\n",
      "18:40:44 | Step 21/492: training loss=1.33\n",
      "18:40:47 | Step 22/492: training loss=1.37\n",
      "18:40:47 | Step 23/492: training loss=3.26\n",
      "18:40:50 | Step 24/492: training loss=2.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51%  ETA 03:44"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:40:50 | Step 25/492: training loss=0.93\n",
      "18:40:52 | Step 26/492: training loss=2.64\n",
      "18:40:52 | Step 27/492: training loss=1.40\n",
      "18:40:55 | Step 28/492: training loss=2.15\n",
      "18:40:55 | Step 29/492: training loss=2.75\n",
      "18:40:58 | Step 30/492: training loss=0.74\n",
      "18:40:58 | Step 31/492: training loss=1.57\n",
      "18:40:58 | Step 32/492: training loss=0.92\n",
      "18:41:01 | Step 33/492: training loss=0.71\n",
      "18:41:01 | Step 34/492: training loss=2.39\n",
      "18:41:04 | Step 35/492: training loss=1.00\n",
      "18:41:04 | Step 36/492: training loss=2.74\n",
      "18:41:07 | Step 37/492: training loss=0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62%  ETA 02:36"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:41:18 | Step 45/492: training loss=0.60\n",
      "18:41:18 | Step 46/492: training loss=0.84\n",
      "18:41:21 | Step 47/492: training loss=0.90\n",
      "18:41:21 | Step 48/492: training loss=2.53\n",
      "18:41:23 | Step 49/492: training loss=0.72\n",
      "18:41:23 | Step 50/492: training loss=0.79\n",
      "18:41:26 | Step 51/492: training loss=1.04\n",
      "18:41:26 | Step 52/492: training loss=1.61\n",
      "18:41:29 | Step 53/492: training loss=0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100%  ETA 00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏁 Job finished with status: succeeded\n",
      "   Fine‑tuned model name → ft:gpt-3.5-turbo-0125:fmai::BO7ZvQM0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === Fine‑tune job with live progress bar ====================================\n",
    "import os, time, json, math, datetime, subprocess, sys\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 1. client\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# 2. upload file\n",
    "clean_path = \"/workspace/data/embeddings/questions/qa_bank_master_openai_v2_clean.jsonl\"\n",
    "print(\"⬆ Uploading file ...\")\n",
    "file_resp = client.files.create(purpose=\"fine-tune\", file=open(clean_path, \"rb\"))\n",
    "file_id   = file_resp.id\n",
    "print(\"✓ File uploaded:\", file_id)\n",
    "\n",
    "# 3. create fine‑tune job\n",
    "print(\"\\n🚀 Launching fine‑tune ...\")\n",
    "job_resp = client.fine_tuning.jobs.create(\n",
    "    model=\"gpt-3.5-turbo-0125\",\n",
    "    training_file=file_id,\n",
    "    hyperparameters={\n",
    "        \"n_epochs\": 3,\n",
    "        \"learning_rate_multiplier\": 0.2\n",
    "    }\n",
    ")\n",
    "job_id = job_resp.id\n",
    "print(\"✓ Job ID:\", job_id)\n",
    "\n",
    "# 4. poll with tqdm bar\n",
    "print(\"\\n⏳ Waiting for completion (updates every 20 s) ...\")\n",
    "pbar = tqdm(total=100, bar_format=\"{l_bar}{bar}| {n_fmt}%  ETA {remaining}\")\n",
    "start_time = time.time()\n",
    "last_events = 0\n",
    "\n",
    "while True:\n",
    "    job = client.fine_tuning.jobs.retrieve(job_id)\n",
    "    events = client.fine_tuning.jobs.list_events(job_id, limit=50).data[::-1]  # oldest→newest\n",
    "    # crude completion estimate: event count / expected (use 30 as heuristic here)\n",
    "    completion = min(99, int(len(events) / 0.8))  # heuristic scale\n",
    "    pbar.n = completion\n",
    "    pbar.refresh()\n",
    "\n",
    "    # print new events\n",
    "    if len(events) > last_events:\n",
    "        for ev in events[last_events:]:\n",
    "            ts = datetime.datetime.fromtimestamp(ev.created_at).strftime(\"%H:%M:%S\")\n",
    "            print(f\"{ts} | {ev.message}\")\n",
    "        last_events = len(events)\n",
    "\n",
    "    if job.status in (\"succeeded\", \"failed\", \"cancelled\"):\n",
    "        break\n",
    "    time.sleep(20)\n",
    "\n",
    "pbar.n = 100\n",
    "pbar.refresh()\n",
    "pbar.close()\n",
    "\n",
    "print(\"\\n🏁 Job finished with status:\", job.status)\n",
    "if job.status == \"succeeded\":\n",
    "    print(\"   Fine‑tuned model name →\", job.fine_tuned_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1decdc6-0af7-4846-b14f-ce57c29bc668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json, pandas as pd, time, os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "NEW_MODEL = \"ft:gpt-3.5-turbo-0125:fmai::BO7ZvQM0\"\n",
    "\n",
    "# --- Load & parse validation JSONL ------------------------------------------\n",
    "rows = [json.loads(l) for l in open(\"/workspace/data/embeddings/questions/qa_finetune.jsonl\")]\n",
    "df_val = pd.DataFrame(rows)\n",
    "\n",
    "def parse_text(blob):\n",
    "    q = re.search(r\"### Question\\n(.*?)\\n\\n### Answer\", blob, re.DOTALL)\n",
    "    a = re.search(r\"### Answer\\n(.*)\", blob, re.DOTALL)\n",
    "    return q.group(1).strip(), a.group(1).strip()\n",
    "\n",
    "df_val[['question', 'gpt4_answer']] = df_val['text'].apply(lambda x: pd.Series(parse_text(x)))\n",
    "\n",
    "# --- Query new fine‑tuned model --------------------------------------------\n",
    "def ask_new(q):\n",
    "    resp = client.chat.completions.create(\n",
    "        model=NEW_MODEL,\n",
    "        messages=[{\"role\":\"user\",\"content\": q}],\n",
    "        temperature=0.0\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "results = []\n",
    "for _, row in df_val.iterrows():\n",
    "    new_ans = ask_new(row[\"question\"])\n",
    "    results.append({\n",
    "        \"question\": row[\"question\"],\n",
    "        \"gpt4_answer\": row[\"gpt4_answer\"],\n",
    "        \"finetuned_v2\": new_ans\n",
    "    })\n",
    "    time.sleep(0.5)  # gentle pacing\n",
    "\n",
    "out_path = \"/workspace/data/embeddings/questions/finetune_validation_results_v2.csv\"\n",
    "pd.DataFrame(results).to_csv(out_path, index=False)\n",
    "print(\"✅ Validation v2 saved to\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "851e56c9-c1a2-485c-be50-c929f60fa984",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The scripts fonttools, pyftmerge, pyftsubset and ttx are installed in '/home/analyst/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"matplotlib\"])\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6d56ca1-b19a-491e-b08b-c6f92061d2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Δ answer length (mean)  ➜ 143.7\n",
      "Δ answer length (median)➜ -66.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAF2CAYAAABzr6yrAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAN3NJREFUeJzt3Xt8TVf+//H3SSQncUlIJCQlhBZ1v6fqUipFvqT0hlbVpVeiqlRbj/kqOh0p7ai2oy6dbzHzc22L3gaTkrgVdZ+ptkilpEhSVEIQJOv3xzxypkeCHfbJhdfz8TiPh7P2Omt9zlnJ9s7e+5zjMMYYAQAA4Kq8SroAAACAsoDQBAAAYAGhCQAAwAJCEwAAgAWEJgAAAAsITQAAABYQmgAAACwgNAEAAFhAaAIAALCA0AR4QOfOndW5c+eSLuOaJk6cKIfDoePHj9s67uXP/+eff5bD4dC8efNsnacw8+bNk8Ph0M8//+xqq127tnr16uXxuSUpKSlJDodDSUlJxTLf5fLy8tS4cWP96U9/KvJji1J7WfkZLy79+/dX3759S7oMeBihCR7xwQcfyOFwKCoqqqRLgaTJkydrxYoVJV1GkX3wwQfFErSuR2mtbdGiRUpNTdWIESNKupRbyiuvvKJPP/1Ue/bsKelS4EGEJnjEggULVLt2bX377bdKTk4u6XJueSUdmmrVqqVz585p4MCBRXrc9QSTgQMH6ty5c6pVq1aRHldUV6qtU6dOOnfunDp16uTR+a/krbfeUv/+/RUYGFgi89+qWrRoodatW+vPf/5zSZcCDyI0wXYpKSn65ptvNG3aNIWEhGjBggUlXZLt8vLydP78+ZIuo8xwOBzy8/OTt7e3x+bIzs6WJHl7e8vPz08Oh8Njc12Nl5eX/Pz85OVV/LvXXbt2ac+ePZwmKiF9+/bVsmXLdObMmZIuBR5CaILtFixYoCpVqqhnz556+OGHCw1N+de4vP3225ozZ47q1q0rp9OpNm3aaNu2bW5909LSNGTIENWoUUNOp1NhYWHq3bu365qV0aNHKzg4WMYY12Oef/55ORwOvffee6629PR0ORwOzZw509WWk5OjCRMm6Pbbb5fT6VTNmjX18ssvKycnx60Gh8OhESNGaMGCBWrUqJGcTqdWrVpVpNelqHOtWLFCjRs3ltPpVKNGjQqdLykpSa1bt5afn5/q1q2r2bNnu65T+v142dnZmj9/vhwOhxwOhwYPHuw2zqlTpzR48GBVrlxZgYGBGjJkiM6ePWvpeeWvn7+/v9q2basNGzYU6FPYNU3XWtfatWtr7969Wrdunavu/Gto8q9bWrdunYYPH67Q0FDVqFHDbdvvr2nK989//lPNmzeXn5+fGjZsqGXLlrltv/y1y3f5mFer7UrXBX388cdq1aqV/P39VbVqVT3++OM6cuSIW5/BgwerYsWKOnLkiPr06aOKFSsqJCREL730knJzc6+wAv+1YsUK+fr6FnqUa9euXYqJiVFAQIAqVqyorl27asuWLdccU7K2xoVp3LixunTpUqA9Ly9Pt912mx5++GFX2+LFi9WqVStVqlRJAQEBatKkid59992rjl+U/Ygk/fjjj3r44YcVFBQkPz8/tW7dWp9//rlr+6lTp+Tt7e223zh+/Li8vLwK7GOGDRum6tWru41/3333KTs7WwkJCdd+cVAmlSvpAnDzWbBggR588EH5+vrq0Ucf1cyZM7Vt2za1adOmQN+FCxfq9OnTevbZZ+VwODR16lQ9+OCDOnjwoHx8fCRJDz30kPbu3avnn39etWvXVkZGhhISEnT48GHVrl1bHTt21DvvvKO9e/eqcePGkqQNGzbIy8tLGzZs0MiRI11tklz/oeTl5en+++/Xxo0b9cwzz+jOO+/Uv//9b73zzjvav39/gdNZa9eu1dKlSzVixAhVrVpVtWvXtvyaFHWujRs3atmyZRo+fLgqVaqk9957Tw899JAOHz6s4OBgSf/5T7BHjx4KCwvTpEmTlJubq9dff10hISFuY/3973/XU089pbZt2+qZZ56RJNWtW9etT9++fRUZGan4+Hjt3LlTf/3rXxUaGqopU6Zc9Xn93//9n5599lndfffdGjVqlA4ePKj7779fQUFBqlmz5lUfe611nT59up5//nlVrFhRf/jDHyRJ1apVcxtj+PDhCgkJ0WuvveY60nQlBw4cUL9+/fTcc89p0KBBmjt3rh555BGtWrVK991331Ufezkrtf3evHnzNGTIELVp00bx8fFKT0/Xu+++q02bNmnXrl2qXLmyq29ubq66d++uqKgovf322/r666/15z//WXXr1tWwYcOuWtc333yjxo0bu3538u3du1cdO3ZUQECAXn75Zfn4+Gj27Nnq3Lmz1q1bd9VrD29kjfv166eJEycqLS3NLWBs3LhRR48eVf/+/SVJCQkJevTRR9W1a1fXz9wPP/ygTZs26YUXXrjqHJK1/cjevXvVvn173XbbbXr11VdVoUIFLV26VH369NGnn36qBx54QJUrV1bjxo21fv16135j48aNcjgcOnnypL7//ns1atRI0n/2Jx07dnSro2HDhvL399emTZv0wAMPXLNulEEGsNH27duNJJOQkGCMMSYvL8/UqFHDvPDCC279UlJSjCQTHBxsTp486Wr/7LPPjCTzxRdfGGOM+e2334wk89Zbb11xzoyMDCPJfPDBB8YYY06dOmW8vLzMI488YqpVq+bqN3LkSBMUFGTy8vKMMcb8/e9/N15eXmbDhg1u482aNctIMps2bXK1STJeXl5m7969ll6He+65x9xzzz2u+0Wdy9fX1yQnJ7va9uzZYySZ999/39UWGxtrypcvb44cOeJqO3DggClXrpy5/Fe7QoUKZtCgQQXqnDBhgpFkhg4d6tb+wAMPmODg4Ks+xwsXLpjQ0FDTvHlzk5OT42qfM2eOkeT2/PPXe+7cucYYa+tqjDGNGjVyGyff3LlzjSTToUMHc+nSpUK3paSkuNpq1aplJJlPP/3U1ZaZmWnCwsJMixYtXG35r8eV5vv9mFeqLTEx0UgyiYmJxpj/vk6NGzc2586dc/X78ssvjSTz2muvudoGDRpkJJnXX3/dbcwWLVqYVq1aFZjrcjVq1DAPPfRQgfY+ffoYX19f89NPP7najh49aipVqmQ6dep0zdqtrHFh9u3bV+Dn1hhjhg8fbipWrGjOnj1rjDHmhRdeMAEBAQXW8lqs7keMMaZr166mSZMm5vz58662vLw8c/fdd5s77rjD1RYXF+e23xg9erTp1KmTCQ0NNTNnzjTGGHPixAnjcDjMu+++W6CmevXqmZiYmCI9D5QdnJ6DrRYsWKBq1aq5Dsk7HA7169dPixcvLvT0Qr9+/VSlShXX/fy/3A4ePChJ8vf3l6+vr5KSkvTbb78VOmdISIgaNGig9evXS5I2bdokb29vjR07Vunp6Tpw4ICk//xl2KFDB9fpl48//lh33nmnGjRooOPHj7tu9957ryQpMTHRbZ577rlHDRs2vK7XpahzRUdHux0Natq0qQICAlyvS25urr7++mv16dNH4eHhrn633367YmJiilzfc88953a/Y8eOOnHihLKysq74mO3btysjI0PPPfecfH19Xe2DBw++5kXIVtbViqefftrydVLh4eFuf/0HBAToiSee0K5du5SWlnbdNVxL/us0fPhw+fn5udp79uypBg0a6KuvvirwmMLWI3/tr+bEiRNuv0/Sf35W/vnPf6pPnz6qU6eOqz0sLEyPPfaYNm7ceMV1vpE1lqR69eqpefPmWrJkiVs9n3zyiWJjY+Xv7y9Jqly58g2d1rrWfuTkyZNau3at+vbtq9OnT7t+/06cOKHu3bvrwIEDrlOlHTt2VHp6uvbt2yfpP/uNTp06qWPHjq6j1Rs3bpQxpsCRJkmqUqWK7R/hgdKD0ATb5ObmavHixerSpYtSUlKUnJys5ORkRUVFKT09XWvWrCnwmIiICLf7+Tu+/P9InU6npkyZopUrV6patWrq1KmTpk6dWuA/ud/v0DZs2KDWrVurdevWCgoK0oYNG5SVlaU9e/a47eQOHDigvXv3KiQkxO1Wr149SVJGRobbHJGRkdf92hR1rstfl/zXJv91ycjI0Llz53T77bcX6FdY27Vcax0Kc+jQIUnSHXfc4dbu4+Pj9p9zYayu67UUZU1uv/32Atcr5b/+hV3/ZJf816l+/foFtjVo0MC1PZ+fn1+BU6y/X/trMb+77kaSfv31V509e7bQ+e+8807l5eUpNTX1qrVfzxrn69evnzZt2uQKJUlJScrIyFC/fv1cfYYPH6569eopJiZGNWrU0NChQ4t0zeC1fn6Tk5NljNH48eML/A5OmDBB0n9/B/P3ERs2bFB2drZ27dqljh07qlOnTm77mICAADVr1qxALcaYEnsTAjyPa5pgm7Vr1+rYsWNavHixFi9eXGD7ggUL1K1bN7e2Kx0l+P2Of9SoUYqNjdWKFSu0evVqjR8/XvHx8Vq7dq1atGghSerQoYM+/PBDHTx40HWtgcPhUIcOHbRhwwaFh4crLy/PLTTl5eWpSZMmmjZtWqE1XH69Rv5fxdejqHNZeV3sVNzzSdbW9VpuZE0Kc6X/7KxchG2XG3mHYXBw8A0dufOEfv36ady4cfr44481atQoLV26VIGBgerRo4erT2hoqHbv3q3Vq1dr5cqVWrlypebOnasnnnhC8+fPv+Yc1/r5zcvLkyS99NJL6t69e6F98//YCA8PV2RkpNavX6/atWvLGKN27dopJCREL7zwgg4dOqQNGzbo7rvvLvQdkr/99luBkImbB6EJtlmwYIFCQ0M1Y8aMAtuWLVum5cuXa9asWdf1H13dunU1ZswYjRkzRgcOHFDz5s315z//Wf/v//0/Sf/96zAhIUHbtm3Tq6++Kuk/F33PnDlT4eHhqlChglq1auU25p49e9S1a1eP/2Vo91yhoaHy8/Mr9DOwCmvzxPPL/xykAwcOuE4zStLFixeVkpJS6F/hl7vWutpZd/7Rht+PuX//fklyXdSff4Ti1KlTbhdnX340qCi15b9O+/btc3ud8tvs/DypBg0aKCUlxa0tJCRE5cuXd51u+r0ff/xRXl5eV7yg2441joyMVNu2bbVkyRKNGDFCy5YtU58+feR0Ot36+fr6KjY2VrGxscrLy9Pw4cM1e/ZsjR8//rqOnv5e/lExHx8fRUdHX7N/x44dtX79ekVGRqp58+aqVKmSmjVrpsDAQK1atUo7d+7UpEmTCjzu0qVLSk1N1f33339D9aL04vQcbHHu3DktW7ZMvXr10sMPP1zgNmLECJ0+fdrt7b1WnD17tsDnIdWtW1eVKlVye6t+ZGSkbrvtNr3zzju6ePGi2rdvL+k/O7+ffvpJn3zyie666y6VK/ffvxP69u2rI0eO6MMPPyz0+Vzr3VhFYfdc3t7eio6O1ooVK3T06FFXe3JyslauXFmgf4UKFXTq1Kki1301rVu3VkhIiGbNmqULFy642ufNm3fNuayuq511Hz16VMuXL3fdz8rK0t/+9jc1b97c9c6u/OvI8q+Pk+T6uIbLWa2tdevWCg0N1axZs9ye28qVK/XDDz+oZ8+e1/uUCmjXrp2+++47t3m8vb3VrVs3ffbZZ26nIdPT07Vw4UJ16NBBAQEBV6z9etf49/r166ctW7boo48+0vHjx91OzUn/uRbr97y8vNS0aVNJKvCRHNcjNDRUnTt31uzZs3Xs2LEC23/99Ve3+x07dtTPP/+sJUuWuP4g8/Ly0t13361p06bp4sWLhV7P9P333+v8+fO6++67b7hmlE4caYItPv/8c50+ffqKf2Hdddddrg+6vHyHeTX79+9X165d1bdvXzVs2FDlypXT8uXLlZ6e7nq7cr6OHTtq8eLFatKkieuIQcuWLVWhQgXt379fjz32mFv/gQMHaunSpXruueeUmJio9u3bKzc3Vz/++KOWLl2q1atXq3Xr1kV8JQrnibkmTpyof/7zn2rfvr2GDRum3Nxc/eUvf1Hjxo21e/dut76tWrXS119/rWnTprlOP9zoV9z4+PjojTfe0LPPPqt7771X/fr1U0pKiubOnXvN612srmurVq00c+ZMvfHGG7r99tsVGhpa4GiNVfXq1dOTTz6pbdu2qVq1avroo4+Unp6uuXPnuvp069ZNERERevLJJzV27Fh5e3vro48+UkhIiA4fPuw2ntXafHx8NGXKFA0ZMkT33HOPHn30UddHDtSuXVsvvvjidT2fwvTu3Vt//OMftW7dOrdT4W+88YYSEhLUoUMHDR8+XOXKldPs2bOVk5OjqVOnXnG8G1nj3+vbt69eeuklvfTSSwoKCipwtOepp57SyZMnde+996pGjRo6dOiQ3n//fTVv3lx33nln0V+IQsyYMUMdOnRQkyZN9PTTT6tOnTpKT0/X5s2b9csvv7h9/Ul+INq3b58mT57sau/UqZNWrlzp+iyoyyUkJKh8+fJF/ggLlCEl9bY93FxiY2ONn5+fyc7OvmKfwYMHGx8fH3P8+HHXW4ULe8u5JDNhwgRjjDHHjx83cXFxpkGDBqZChQomMDDQREVFmaVLlxZ43IwZM4wkM2zYMLf26OhoI8msWbOmwGMuXLhgpkyZYho1amScTqepUqWKadWqlZk0aZLJzMx0qykuLs7qy1HgIwfsmKtWrVoFPjZgzZo1pkWLFsbX19fUrVvX/PWvfzVjxowxfn5+bv1+/PFH06lTJ+Pv728kucbJf4v9r7/+6ta/sLfYX8kHH3xgIiMjjdPpNK1btzbr168v8Pwv/8gBq+ualpZmevbsaSpVquT2Fvf8+rZt21agnit95EDPnj3N6tWrTdOmTY3T6TQNGjQwH3/8cYHH79ixw0RFRRlfX18TERFhpk2bVuiYV6rt8rft51uyZIlp0aKFcTqdJigoyAwYMMD88ssvbn0GDRpkKlSoUKCmK30UQmGaNm1qnnzyyQLtO3fuNN27dzcVK1Y05cuXN126dDHffPONW58r1W5lja+lffv2RpJ56qmnCmz75JNPTLdu3UxoaKjrdX/22WfNsWPHrjqm1f1Ivp9++sk88cQTpnr16sbHx8fcdtttplevXuaTTz4p8PjQ0FAjyaSnp7vaNm7caCSZjh07FlpPVFSUefzxx69aM8o2hzEevNITQLHr06eP9u7d6/qoBdxa/v73vysuLk6HDx92uy4LnrV79261bNlSO3fuVPPmzUu6HHgI1zQBZdi5c+fc7h84cED/+Mc/XF/pgVvPgAEDFBERUegbMuA5b775ph5++GEC002OI01AGRYWFqbBgwerTp06OnTokGbOnKmcnBzt2rWLtz0DgM24EBwow3r06KFFixYpLS1NTqdT7dq10+TJkwlMAOABHGkCAACwgGuaAAAALCA0AQAAWFDqrmnKy8vT0aNHValSJb70EAAAeJwxRqdPn1Z4eHih3ymYr9SFpqNHj17xe5AAAAA8JTU1VTVq1Lji9lIXmipVqiTpP4Vf6fuQAAAA7JKVlaWaNWu6MsiVlLrQlH9KLiAggNAEAACKzbUuC+JCcAAAAAsITQAAABYQmgAAACwoddc0AQAA++Xm5urixYslXUaJ8PHxkbe39w2PQ2gCAOAmZoxRWlqaTp06VdKllKjKlSurevXqN/QZkIQmAABuYvmBKTQ0VOXLl7/lPjjaGKOzZ88qIyNDkhQWFnbdYxGaAAC4SeXm5roCU3BwcEmXU2L8/f0lSRkZGQoNDb3uU3VcCA4AwE0q/xqm8uXLl3AlJS//NbiR67oITQAA3ORutVNyhbHjNSA0AQAAWEBoAgAAsIALwQEAuMW8k7C/WOd78b56to537NgxjRkzRtu3b1dycrJGjhyp6dOn2zpHYQhNgB0S4z0/R5dxnp8DAMqAnJwchYSE6H//93/1zjvvFNu8nJ4DAAClypw5cxQeHq68vDy39t69e2vo0KGqXbu23n33XT3xxBMKDAwstroITQAAoFR55JFHdOLECSUmJrraTp48qVWrVmnAgAElVhehCQAAlCpVqlRRTEyMFi5c6Gr75JNPVLVqVXXp0qXE6iI0AQCAUmfAgAH69NNPlZOTI0lasGCB+vfvLy+vkosuhCYAAFDqxMbGyhijr776SqmpqdqwYUOJnpqTePccAAAohfz8/PTggw9qwYIFSk5OVv369dWyZcsSrYnQBAAASqUBAwaoV69e2rt3rx5//HG3bbt375YknTlzRr/++qt2794tX19fNWzY0GP1EJoAAECpdO+99yooKEj79u3TY4895ratRYsWrn/v2LFDCxcuVK1atfTzzz97rB5CEwAAtxi7P6HbU7y8vHT06NFCtxljirkaLgQHAACwhNAEAABgAaEJAADAgiKHpvXr1ys2Nlbh4eFyOBxasWJFgT4//PCD7r//fgUGBqpChQpq06aNDh8+bEe9AAAAJaLIoSk7O1vNmjXTjBkzCt3+008/qUOHDmrQoIGSkpL0r3/9S+PHj5efn98NFwsAAFBSivzuuZiYGMXExFxx+x/+8Af9z//8j6ZOnepqq1u37vVVBwAAUErYek1TXl6evvrqK9WrV0/du3dXaGiooqKiCj2Fly8nJ0dZWVluNwAAgNLG1s9pysjI0JkzZ/Tmm2/qjTfe0JQpU7Rq1So9+OCDSkxM1D333FPgMfHx8Zo0aZKdZdywdxL22zZWWfksDAAAcHW2H2mSpN69e+vFF19U8+bN9eqrr6pXr16aNWtWoY8ZN26cMjMzXbfU1FQ7SwIAALCFrUeaqlatqnLlyhX43pc777xTGzduLPQxTqdTTqfTzjIAAABsZ2to8vX1VZs2bbRv3z639v3796tWrVp2TgUAAK5XYnzxztdlnK3DLVu2TDNnztTu3buVk5OjRo0aaeLEierevbut81yuyKHpzJkzSk5Odt1PSUnR7t27FRQUpIiICI0dO1b9+vVTp06d1KVLF61atUpffPGFkpKS7KwbAADcotavX6/77rtPkydPVuXKlTV37lzFxsZq69atbl/ka7cih6bt27erS5curvujR4+WJA0aNEjz5s3TAw88oFmzZik+Pl4jR45U/fr19emnn6pDhw72VQ0AAG5ac+bM0cSJE/XLL7/Iy+u/l1/37t1bwcHB+uijj9z6T548WZ999pm++OKL0hWaOnfufM1vFh46dKiGDh163UUBAIBb1yOPPKLnn39eiYmJ6tq1qyTp5MmTWrVqlf7xj38U6J+Xl6fTp08rKCjIo3Xx3XMAAKBUqVKlimJiYrRw4UJX2yeffKKqVau6ne3K9/bbb+vMmTPq27evR+siNAEAgFJnwIAB+vTTT5WTkyNJWrBggfr37+92uk6SFi5cqEmTJmnp0qUKDQ31aE2EJgAAUOrExsbKGKOvvvpKqamp2rBhgwYMGODWZ/HixXrqqae0dOlSRUdHe7wmWz9yAAAAwA5+fn568MEHtWDBAiUnJ6t+/fpq2bKla/uiRYs0dOhQLV68WD179iyWmghNAACgVBowYIB69eqlvXv36vHHH3e1L1y4UIMGDdK7776rqKgopaWlSZL8/f0VGBjosXo4PQcAAEqle++9V0FBQdq3b58ee+wxV/ucOXN06dIlxcXFKSwszHV74YUXPFoPR5oAALjV2PwJ3Z7i5eWlo0ePFmgvqQ/M5kgTAACABYQmAAAACwhNAAAAFnBNE3ALeidhvy3jvHhfPVvGAYCygCNNAAAAFhCaAAC4yeXl5ZV0CSXOjteA03MAANykfH19XW/bDwkJka+vrxwOR0mXVayMMbpw4YJ+/fVXeXl5ydfX97rHIjQBAHCT8vLyUmRkpI4dO1bo5x3dSsqXL6+IiIgCX/hbFIQmAABuYr6+voqIiNClS5eUm5tb0uWUCG9vb5UrV+6Gj7IRmgAAuMk5HA75+PjIx8enpEsp07gQHAAAwAJCEwAAgAWEJgAAAAsITQAAABYQmgAAACwgNAEAAFhQ5NC0fv16xcbGKjw8XA6HQytWrLhi3+eee04Oh0PTp0+/gRIBAABKXpFDU3Z2tpo1a6YZM2Zctd/y5cu1ZcsWhYeHX3dxAAAApUWRP9wyJiZGMTExV+1z5MgRPf/881q9erV69ux53cUBAACUFrZf05SXl6eBAwdq7NixatSokd3DAwAAlAjbv0ZlypQpKleunEaOHGmpf05OjnJyclz3s7Ky7C4JAADghtl6pGnHjh169913NW/ePMtfihcfH6/AwEDXrWbNmnaWBAAAYAtbQ9OGDRuUkZGhiIgIlStXTuXKldOhQ4c0ZswY1a5du9DHjBs3TpmZma5bamqqnSUBAADYwtbTcwMHDlR0dLRbW/fu3TVw4EANGTKk0Mc4nU45nU47ywAAALBdkUPTmTNnlJyc7LqfkpKi3bt3KygoSBEREQoODnbr7+Pjo+rVq6t+/fo3Xi0AAEAJKXJo2r59u7p06eK6P3r0aEnSoEGDNG/ePNsKAwAAKE2KHJo6d+4sY4zl/j///HNRpwAAACh1+O45AAAACwhNAAAAFhCaAAAALCA0AQAAWEBoAgAAsIDQBAAAYAGhCQAAwAJCEwAAgAWEJgAAAAsITQAAABYQmgAAACwgNAEAAFhAaAIAALCA0AQAAGABoQkAAMACQhMAAIAFhCYAAAALCE0AAAAWEJoAAAAsIDQBAABYQGgCAACwgNAEAABgAaEJAADAAkITAACABUUOTevXr1dsbKzCw8PlcDi0YsUK17aLFy/qlVdeUZMmTVShQgWFh4friSee0NGjR+2sGQAAoNgVOTRlZ2erWbNmmjFjRoFtZ8+e1c6dOzV+/Hjt3LlTy5Yt0759+3T//ffbUiwAAEBJKVfUB8TExCgmJqbQbYGBgUpISHBr+8tf/qK2bdvq8OHDioiIuL4qAQAASliRQ1NRZWZmyuFwqHLlyoVuz8nJUU5Ojut+VlaWp0sCAAAoMo9eCH7+/Hm98sorevTRRxUQEFBon/j4eAUGBrpuNWvW9GRJAAAA18VjoenixYvq27evjDGaOXPmFfuNGzdOmZmZrltqaqqnSgIAALhuHjk9lx+YDh06pLVr117xKJMkOZ1OOZ1OT5QBAABgG9tDU35gOnDggBITExUcHGz3FAAAAMWuyKHpzJkzSk5Odt1PSUnR7t27FRQUpLCwMD388MPauXOnvvzyS+Xm5iotLU2SFBQUJF9fX/sqBwAAKEZFDk3bt29Xly5dXPdHjx4tSRo0aJAmTpyozz//XJLUvHlzt8clJiaqc+fO118pAABACSpyaOrcubOMMVfcfrVtAAAAZRXfPQcAAGABoQkAAMACQhMAAIAFhCYAAAALCE0AAAAWEJoAAAAsIDQBAABYQGgCAACwgNAEAABgAaEJAADAgiJ/jQpgu8R4z47fZZxnxwcA3BI40gQAAGABoQkAAMACQhMAAIAFhCYAAAALCE0AAAAWEJoAAAAsIDQBAABYQGgCAACwgNAEAABgAaEJAADAAkITAACABYQmAAAAC4ocmtavX6/Y2FiFh4fL4XBoxYoVbtuNMXrttdcUFhYmf39/RUdH68CBA3bVCwAAUCKKHJqys7PVrFkzzZgxo9DtU6dO1XvvvadZs2Zp69atqlChgrp3767z58/fcLEAAAAlpVxRHxATE6OYmJhCtxljNH36dP3v//6vevfuLUn629/+pmrVqmnFihXq37//jVULAABQQmy9piklJUVpaWmKjo52tQUGBioqKkqbN2+2cyoAAIBiVeQjTVeTlpYmSapWrZpbe7Vq1VzbLpeTk6OcnBzX/aysLDtLAgAAsIWtoel6xMfHa9KkSSVdBgAU8E7CflvGefG+eraMA6Bk2Xp6rnr16pKk9PR0t/b09HTXtsuNGzdOmZmZrltqaqqdJQEAANjC1tAUGRmp6tWra82aNa62rKwsbd26Ve3atSv0MU6nUwEBAW43AACA0qbIp+fOnDmj5ORk1/2UlBTt3r1bQUFBioiI0KhRo/TGG2/ojjvuUGRkpMaPH6/w8HD16dPHzroBAACKVZFD0/bt29WlSxfX/dGjR0uSBg0apHnz5unll19Wdna2nnnmGZ06dUodOnTQqlWr5OfnZ1/VAAAAxazIoalz584yxlxxu8Ph0Ouvv67XX3/9hgoDAAAoTfjuOQAAAAsITQAAABYQmgAAACwgNAEAAFhAaAIAALCA0AQAAGABoQkAAMACQhMAAIAFhCYAAAALCE0AAAAWEJoAAAAsKPJ3zwEoIYnxtg111+ET9gyUGFy0/l3G2TMvAJQAjjQBAABYQGgCAACwgNAEAABgAaEJAADAAkITAACABYQmAAAACwhNAAAAFhCaAAAALCA0AQAAWEBoAgAAsIDQBAAAYAGhCQAAwALbQ1Nubq7Gjx+vyMhI+fv7q27duvrjH/8oY4zdUwEAABSbcnYPOGXKFM2cOVPz589Xo0aNtH37dg0ZMkSBgYEaOXKk3dMBAAAUC9tD0zfffKPevXurZ8+ekqTatWtr0aJF+vbbb+2eCgAAoNjYfnru7rvv1po1a7R//35J0p49e7Rx40bFxMQU2j8nJ0dZWVluNwAAgNLG9iNNr776qrKystSgQQN5e3srNzdXf/rTnzRgwIBC+8fHx2vSpEl2lwH8V2J8SVeAfFdZi80HT9gyRbsn37ZlHAC4nO1HmpYuXaoFCxZo4cKF2rlzp+bPn6+3335b8+fPL7T/uHHjlJmZ6bqlpqbaXRIAAMANs/1I09ixY/Xqq6+qf//+kqQmTZro0KFDio+P16BBgwr0dzqdcjqddpcBAABgK9uPNJ09e1ZeXu7Dent7Ky8vz+6pAAAAio3tR5piY2P1pz/9SREREWrUqJF27dqladOmaejQoXZPBQAAUGxsD03vv/++xo8fr+HDhysjI0Ph4eF69tln9dprr9k9FQAAQLGxPTRVqlRJ06dP1/Tp0+0eGgAAoMTw3XMAAAAWEJoAAAAsIDQBAABYQGgCAACwgNAEAABgAaEJAADAAkITAACABYQmAAAACwhNAAAAFhCaAAAALLD9a1QA3Do2HzxR0iWUCe8k7LdlnBfvq2fLOACuD0eaAAAALCA0AQAAWEBoAgAAsIDQBAAAYAGhCQAAwAJCEwAAgAWEJgAAAAsITQAAABYQmgAAACwgNAEAAFhAaAIAALCA0AQAAGCBR0LTkSNH9Pjjjys4OFj+/v5q0qSJtm/f7ompAAAAikU5uwf87bff1L59e3Xp0kUrV65USEiIDhw4oCpVqtg9FQAAQLGxPTRNmTJFNWvW1Ny5c11tkZGRdk8DAABQrGw/Pff555+rdevWeuSRRxQaGqoWLVroww8/tHsaAACAYmV7aDp48KBmzpypO+64Q6tXr9awYcM0cuRIzZ8/v9D+OTk5ysrKcrsBAACUNrafnsvLy1Pr1q01efJkSVKLFi303XffadasWRo0aFCB/vHx8Zo0aZLdZQC4VSXG2zbUXYdPFGjbEvGMbeMDKFtsP9IUFhamhg0burXdeeedOnz4cKH9x40bp8zMTNctNTXV7pIAAABumO1Hmtq3b699+/a5te3fv1+1atUqtL/T6ZTT6bS7DAAAAFvZfqTpxRdf1JYtWzR58mQlJydr4cKFmjNnjuLi4uyeCgAAoNjYHpratGmj5cuXa9GiRWrcuLH++Mc/avr06RowYIDdUwEAABQb20/PSVKvXr3Uq1cvTwwNAABQIvjuOQAAAAsITQAAABYQmgAAACwgNAEAAFhAaAIAALCA0AQAAGABoQkAAMACQhMAAIAFhCYAAAALCE0AAAAWEJoAAAAs8Mh3z5UJifFX3HTX4RO2TLEl4hlbxkHx23zQnp8BSWpXJ9i2sQDbXGUfaIsu4zw7PkqXW+TniSNNAAAAFhCaAAAALCA0AQAAWEBoAgAAsIDQBAAAYAGhCQAAwAJCEwAAgAWEJgAAAAsITQAAABYQmgAAACwgNAEAAFhAaAIAALDA46HpzTfflMPh0KhRozw9FQAAgMd4NDRt27ZNs2fPVtOmTT05DQAAgMd5LDSdOXNGAwYM0IcffqgqVap4ahoAAIBi4bHQFBcXp549eyo6Ovqq/XJycpSVleV2AwAAKG3KeWLQxYsXa+fOndq2bds1+8bHx2vSpEmeKKNUeCdhvy3jvHhfPVvGAYDSiv0lSjvbjzSlpqbqhRde0IIFC+Tn53fN/uPGjVNmZqbrlpqaandJAAAAN8z2I007duxQRkaGWrZs6WrLzc3V+vXr9Ze//EU5OTny9vZ2bXM6nXI6nXaXAQAAYCvbQ1PXrl3173//261tyJAhatCggV555RW3wAQAAFBW2B6aKlWqpMaNG7u1VahQQcHBwQXaAQAAygo+ERwAAMACj7x77nJJSUnFMQ0AAIDHcKQJAADAAkITAACABYQmAAAACwhNAAAAFhCaAAAALCA0AQAAWEBoAgAAsIDQBAAAYAGhCQAAwAJCEwAAgAWEJgAAAAuK5bvngKvZfPCEbWO1qxNs21hASbnr8JzCNyTa9/Nt1+8dv3O4lXCkCQAAwAJCEwAAgAWEJgAAAAsITQAAABYQmgAAACwgNAEAAFhAaAIAALCA0AQAAGABoQkAAMACQhMAAIAFhCYAAAALbA9N8fHxatOmjSpVqqTQ0FD16dNH+/bts3saAACAYmV7aFq3bp3i4uK0ZcsWJSQk6OLFi+rWrZuys7PtngoAAKDYlLN7wFWrVrndnzdvnkJDQ7Vjxw516tTJ7ukAAACKhcevacrMzJQkBQUFeXoqAAAAj7H9SNPv5eXladSoUWrfvr0aN25caJ+cnBzl5OS47mdlZXmyJAAAgOvi0dAUFxen7777Ths3brxin/j4eE2aNMmTZcBD3knYb8s4d9kySum1+eCJki4BN4ky9bOUGF/kh9x1uGjPb0vEM0We45Z0HWuBwnns9NyIESP05ZdfKjExUTVq1Lhiv3HjxikzM9N1S01N9VRJAAAA1832I03GGD3//PNavny5kpKSFBkZedX+TqdTTqfT7jIAAABsZXtoiouL08KFC/XZZ5+pUqVKSktLkyQFBgbK39/f7ukAAACKhe2n52bOnKnMzEx17txZYWFhrtuSJUvsngoAAKDYeOT0HAAAwM2G754DAACwgNAEAABgAaEJAADAAkITAACABYQmAAAACwhNAAAAFhCaAAAALCA0AQAAWEBoAgAAsIDQBAAAYAGhCQAAwAKHKWVfFpeVlaXAwEBlZmYqICDAcxMlxl9x0+aDJzw3r422RDxT0iXY4q7Dc0q6BAA3kXZ1gku6BNityziPDm81e3CkCQAAwAJCEwAAgAWEJgAAAAsITQAAABYQmgAAACwgNAEAAFhAaAIAALCA0AQAAGABoQkAAMACQhMAAIAFhCYAAAALCE0AAAAWeCw0zZgxQ7Vr15afn5+ioqL07bffemoqAAAAj/NIaFqyZIlGjx6tCRMmaOfOnWrWrJm6d++ujIwMT0wHAADgcR4JTdOmTdPTTz+tIUOGqGHDhpo1a5bKly+vjz76yBPTAQAAeFw5uwe8cOGCduzYoXHjxrnavLy8FB0drc2bNxfon5OTo5ycHNf9zMxMSVJWVpbdpbnLPn/lTedyrritNDmffaakS7BFWXm9AZQNWVfZv6OM8nAmyM8cxpir9rM9NB0/fly5ubmqVq2aW3u1atX0448/FugfHx+vSZMmFWivWbOm3aXdhP5S0gUAAFAMXi+WWU6fPq3AwMArbrc9NBXVuHHjNHr0aNf9vLw8nTx5UsHBwXI4HG59s7KyVLNmTaWmpiogIKC4S8UNYO3KJtatbGLdyibWreQYY3T69GmFh4dftZ/toalq1ary9vZWenq6W3t6erqqV69eoL/T6ZTT6XRrq1y58lXnCAgI4AeqjGLtyibWrWxi3com1q1kXO0IUz7bLwT39fVVq1attGbNGldbXl6e1qxZo3bt2tk9HQAAQLHwyOm50aNHa9CgQWrdurXatm2r6dOnKzs7W0OGDPHEdAAAAB7nkdDUr18//frrr3rttdeUlpam5s2ba9WqVQUuDi8qp9OpCRMmFDidh9KPtSubWLeyiXUrm1i30s9hrvX+OgAAAPDdcwAAAFYQmgAAACwgNAEAAFhAaAIAALCgTIWmGTNmqHbt2vLz81NUVJS+/fbbki7pljVx4kQ5HA63W4MGDVzbz58/r7i4OAUHB6tixYp66KGHCnzg6eHDh9WzZ0+VL19eoaGhGjt2rC5dulTcT+Wmt379esXGxio8PFwOh0MrVqxw226M0WuvvaawsDD5+/srOjpaBw4ccOtz8uRJDRgwQAEBAapcubKefPJJnTnj/t2H//rXv9SxY0f5+fmpZs2amjp1qqef2k3tWus2ePDgAr+DPXr0cOvDuhW/+Ph4tWnTRpUqVVJoaKj69Omjffv2ufWxa/+YlJSkli1byul06vbbb9e8efM8/fRueWUmNC1ZskSjR4/WhAkTtHPnTjVr1kzdu3dXRkZGSZd2y2rUqJGOHTvmum3cuNG17cUXX9QXX3yhjz/+WOvWrdPRo0f14IMPurbn5uaqZ8+eunDhgr755hvNnz9f8+bN02uvvVYST+Wmlp2drWbNmmnGjBmFbp86daree+89zZo1S1u3blWFChXUvXt3nT//3y89HTBggPbu3auEhAR9+eWXWr9+vZ555hnX9qysLHXr1k21atXSjh079NZbb2nixImaM2eOx5/fzepa6yZJPXr0cPsdXLRokdt21q34rVu3TnFxcdqyZYsSEhJ08eJFdevWTdnZ2a4+duwfU1JS1LNnT3Xp0kW7d+/WqFGj9NRTT2n16tXF+nxvOaaMaNu2rYmLi3Pdz83NNeHh4SY+Pr4Eq7p1TZgwwTRr1qzQbadOnTI+Pj7m448/drX98MMPRpLZvHmzMcaYf/zjH8bLy8ukpaW5+sycOdMEBASYnJwcj9Z+K5Nkli9f7rqfl5dnqlevbt566y1X26lTp4zT6TSLFi0yxhjz/fffG0lm27Ztrj4rV640DofDHDlyxBhjzAcffGCqVKnitnavvPKKqV+/voef0a3h8nUzxphBgwaZ3r17X/ExrFvpkJGRYSSZdevWGWPs2z++/PLLplGjRm5z9evXz3Tv3t3TT+mWViaONF24cEE7duxQdHS0q83Ly0vR0dHavHlzCVZ2aztw4IDCw8NVp04dDRgwQIcPH5Yk7dixQxcvXnRbrwYNGigiIsK1Xps3b1aTJk3cPvC0e/fuysrK0t69e4v3idzCUlJSlJaW5rZWgYGBioqKclurypUrq3Xr1q4+0dHR8vLy0tatW119OnXqJF9fX1ef7t27a9++ffrtt9+K6dncepKSkhQaGqr69etr2LBhOnHihGsb61Y6ZGZmSpKCgoIk2bd/3Lx5s9sY+X34P9GzykRoOn78uHJzcwt8oni1atWUlpZWQlXd2qKiojRv3jytWrVKM2fOVEpKijp27KjTp08rLS1Nvr6+Bb54+ffrlZaWVuh65m9D8ch/ra/2u5WWlqbQ0FC37eXKlVNQUBDrWYJ69Oihv/3tb1qzZo2mTJmidevWKSYmRrm5uZJYt9IgLy9Po0aNUvv27dW4cWNJsm3/eKU+WVlZOnfunCeeDuShr1HBzS8mJsb176ZNmyoqKkq1atXS0qVL5e/vX4KVAbeG/v37u/7dpEkTNW3aVHXr1lVSUpK6du1agpUhX1xcnL777ju36z1RtpWJI01Vq1aVt7d3gXcXpKenq3r16iVUFX6vcuXKqlevnpKTk1W9enVduHBBp06dcuvz+/WqXr16oeuZvw3FI/+1vtrvVvXq1Qu84eLSpUs6efIk61mK1KlTR1WrVlVycrIk1q2kjRgxQl9++aUSExNVo0YNV7td+8cr9QkICOAPVw8qE6HJ19dXrVq10po1a1xteXl5WrNmjdq1a1eClSHfmTNn9NNPPyksLEytWrWSj4+P23rt27dPhw8fdq1Xu3bt9O9//9ttp56QkKCAgAA1bNiw2Ou/VUVGRqp69epua5WVlaWtW7e6rdWpU6e0Y8cOV5+1a9cqLy9PUVFRrj7r16/XxYsXXX0SEhJUv359ValSpZieza3tl19+0YkTJxQWFiaJdSspxhiNGDFCy5cv19q1axUZGem23a79Y7t27dzGyO/D/4keVtJXolu1ePFi43Q6zbx588z3339vnnnmGVO5cmW3dxeg+IwZM8YkJSWZlJQUs2nTJhMdHW2qVq1qMjIyjDHGPPfccyYiIsKsXbvWbN++3bRr1860a9fO9fhLly6Zxo0bm27dupndu3ebVatWmZCQEDNu3LiSeko3rdOnT5tdu3aZXbt2GUlm2rRpZteuXebQoUPGGGPefPNNU7lyZfPZZ5+Zf/3rX6Z3794mMjLSnDt3zjVGjx49TIsWLczWrVvNxo0bzR133GEeffRR1/ZTp06ZatWqmYEDB5rvvvvOLF682JQvX97Mnj272J/vzeJq63b69Gnz0ksvmc2bN5uUlBTz9ddfm5YtW5o77rjDnD9/3jUG61b8hg0bZgIDA01SUpI5duyY63b27FlXHzv2jwcPHjTly5c3Y8eONT/88IOZMWOG8fb2NqtWrSrW53urKTOhyRhj3n//fRMREWF8fX1N27ZtzZYtW0q6pFtWv379TFhYmPH19TW33Xab6devn0lOTnZtP3funBk+fLipUqWKKV++vHnggQfMsWPH3Mb4+eefTUxMjPH39zdVq1Y1Y8aMMRcvXizup3LTS0xMNJIK3AYNGmSM+c/HDowfP95Uq1bNOJ1O07VrV7Nv3z63MU6cOGEeffRRU7FiRRMQEGCGDBliTp8+7dZnz549pkOHDsbpdJrbbrvNvPnmm8X1FG9KV1u3s2fPmm7dupmQkBDj4+NjatWqZZ5++ukCf0SybsWvsDWTZObOnevqY9f+MTEx0TRv3tz4+vqaOnXquM0Bz3AYY0xxH90CAAAoa8rENU0AAAAljdAEAABgAaEJAADAAkITAACABYQmAAAACwhNAAAAFhCaAAAALCA0AQAAWEBoAgAAsIDQBAAAYAGhCQAAwAJCEwAAgAX/Hz8Xq/S8BN6+AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 700x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msm\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\n\u001b[1;32m     33\u001b[0m model \u001b[38;5;241m=\u001b[39m sentence_transformers\u001b[38;5;241m.\u001b[39mSentenceTransformer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m emb_new \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnew_a\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m emb_ref \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;28mlist\u001b[39m(new[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt4_answer\u001b[39m\u001b[38;5;124m'\u001b[39m]), batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     37\u001b[0m sim \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39mcosine_similarity(emb_new, emb_ref)\u001b[38;5;241m.\u001b[39mdiagonal()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/SentenceTransformer.py:601\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 601\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    603\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/SentenceTransformer.py:668\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m    666\u001b[0m     module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[1;32m    667\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[0;32m--> 668\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/models/Transformer.py:118\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m    116\u001b[0m     trans_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 118\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    121\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_tokens, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:961\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    957\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    959\u001b[0m \u001b[38;5;66;03m# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;66;03m# ourselves in which case we just need to make it broadcastable to all heads.\u001b[39;00m\n\u001b[0;32m--> 961\u001b[0m extended_attention_mask: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_extended_attention_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;66;03m# If a 2D or 3D attention mask is provided for the cross-attention\u001b[39;00m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;66;03m# we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\u001b[39;00m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;129;01mand\u001b[39;00m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:1078\u001b[0m, in \u001b[0;36mModuleUtilsMixin.get_extended_attention_mask\u001b[0;34m(self, attention_mask, input_shape, device, dtype)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1070\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong shape for input_ids (shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) or attention_mask (shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattention_mask\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1071\u001b[0m     )\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;66;03m# Since attention_mask is 1.0 for positions we want to attend and 0.0 for\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;66;03m# masked positions, this operation will create a tensor which is 0.0 for\u001b[39;00m\n\u001b[1;32m   1075\u001b[0m \u001b[38;5;66;03m# positions we want to attend and the dtype's smallest value for masked positions.\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;66;03m# Since we are adding it to the raw scores before the softmax, this is\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m \u001b[38;5;66;03m# effectively the same as removing these entirely.\u001b[39;00m\n\u001b[0;32m-> 1078\u001b[0m extended_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[43mextended_attention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# fp16 compatibility\u001b[39;00m\n\u001b[1;32m   1079\u001b[0m extended_attention_mask \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m extended_attention_mask) \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(dtype)\u001b[38;5;241m.\u001b[39mmin\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extended_attention_mask\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "old = pd.read_csv(\"/workspace/data/embeddings/questions/finetune_validation_results_corrected.csv\")\n",
    "new = pd.read_csv(\"/workspace/data/embeddings/questions/finetune_validation_results_v2.csv\")\n",
    "\n",
    "# --- Align column names ------------------------------------------------------\n",
    "old_q = 'question'\n",
    "old_a = 'finetuned_answer'\n",
    "\n",
    "new_q = 'question'\n",
    "new_a = 'finetuned_v2'          # <- check: should match your CSV header\n",
    "\n",
    "# Basic sanity\n",
    "assert all(col in old.columns for col in [old_q, old_a])\n",
    "assert all(col in new.columns for col in [new_q, new_a])\n",
    "\n",
    "# --- Length comparison -------------------------------------------------------\n",
    "old['v1_len'] = old[old_a].str.len()\n",
    "new['v2_len'] = new[new_a].str.len()\n",
    "\n",
    "print(\"Δ answer length (mean)  ➜\", round(new['v2_len'].mean() - old['v1_len'].mean(), 1))\n",
    "print(\"Δ answer length (median)➜\", round(new['v2_len'].median() - old['v1_len'].median(), 1))\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.hist(old['v1_len'], bins=20, alpha=0.5, label='v1')\n",
    "plt.hist(new['v2_len'], bins=20, alpha=0.5, label='v2')\n",
    "plt.legend(); plt.title(\"Answer length distribution (old vs new)\")\n",
    "plt.show()\n",
    "\n",
    "# --- Quick cosine similarity snapshot (optional) ----------------------------\n",
    "import numpy as np, sklearn.metrics.pairwise as sm, sentence_transformers\n",
    "\n",
    "model = sentence_transformers.SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "emb_new = model.encode(list(new[new_a]), batch_size=32, show_progress_bar=False)\n",
    "emb_ref = model.encode(list(new['gpt4_answer']), batch_size=32, show_progress_bar=False)\n",
    "\n",
    "sim = sm.cosine_similarity(emb_new, emb_ref).diagonal()\n",
    "print(\"Mean cosine similarity vs GPT‑4:\", sim.mean().round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ffb1fc17-581c-4bba-9136-82f115b138f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cosine similarity vs GPT‑4: 0.765\n"
     ]
    }
   ],
   "source": [
    "import sentence_transformers, torch, numpy as np, sklearn.metrics.pairwise as sm\n",
    "\n",
    "device = \"cpu\"   # force CPU to avoid the GPU kernel issue\n",
    "model = sentence_transformers.SentenceTransformer(\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\", device=device\n",
    ")\n",
    "\n",
    "emb_new = model.encode(list(new[new_a]), batch_size=32, show_progress_bar=False, device=device)\n",
    "emb_ref = model.encode(list(new['gpt4_answer']), batch_size=32, show_progress_bar=False, device=device)\n",
    "\n",
    "sim = sm.cosine_similarity(emb_new, emb_ref).diagonal()\n",
    "print(\"Mean cosine similarity vs GPT‑4:\", round(sim.mean(), 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84ff3830-7eab-403a-9426-281925aad168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P90 v1: 1450.6000000000001 | P90 v2: 1799.4000000000005\n"
     ]
    }
   ],
   "source": [
    "print(\"P90 v1:\", old['v1_len'].quantile(0.9),\n",
    "      \"| P90 v2:\", new['v2_len'].quantile(0.9))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f298dbe-6edf-4081-a20d-a50f459ad5b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
