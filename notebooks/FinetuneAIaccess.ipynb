{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b2c7002-5fe4-48fd-abda-0230dbb25f63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded API key: True\n",
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(f\"Loaded API key: {api_key is not None}\")  # Must print True\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello, test.\"}]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29c160e2-afd7-4520-8074-cd9f3faed21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in /home/analyst/.local/lib/python3.10/site-packages (1.75.0)\n",
      "Requirement already satisfied: python-dotenv in /home/analyst/.local/lib/python3.10/site-packages (1.1.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/analyst/.local/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/analyst/.local/lib/python3.10/site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/analyst/.local/lib/python3.10/site-packages (from openai) (2.11.3)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/analyst/.local/lib/python3.10/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/analyst/.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /home/analyst/.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/analyst/.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0abacd43-3f71-4cc7-8dc1-d955adb1602c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .env\n"
     ]
    }
   ],
   "source": [
    "%%writefile .env\n",
    "OPENAI_API_KEY=\"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "103a37da-551c-400f-bf6d-bb0ce20b9b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# initialize client explicitly\n",
    "client = OpenAI(api_key=os.getenv(\"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\"))\n",
    "\n",
    "# perform chat completion using client.chat.completions\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello, OpenAI!\"}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6888e7d-b589-49e8-815c-ceb6993072ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load your API key from the .env file\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Specify the correct file path\n",
    "file_path = \"/workspace/data/embeddings/questions/qa_bank_master.jsonl\"\n",
    "\n",
    "# Upload the training file\n",
    "with open(file_path, \"rb\") as f:\n",
    "    training_file = client.files.create(file=f, purpose=\"fine-tune\")\n",
    "\n",
    "print(f\"Uploaded file ID: {training_file.id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae4a69a-999d-402f-95ad-0ddb9eb7eb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the fine-tuning job\n",
    "fine_tune_job = client.fine_tuning.jobs.create(\n",
    "    training_file=training_file.id,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    suffix=\"ai-trading-qa\"\n",
    ")\n",
    "\n",
    "print(f\"Fine-tuning job ID: {fine_tune_job.id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc2bd34-708f-4903-8ec0-46b8a394e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Check the status of the fine-tuning job\n",
    "status = client.fine_tuning.jobs.retrieve(fine_tune_job.id).status\n",
    "print(f\"Current status: {status}\")\n",
    "\n",
    "# Wait until the job is completed\n",
    "while status not in [\"succeeded\", \"failed\"]:\n",
    "    time.sleep(10)  # Wait for 10 seconds before checking again\n",
    "    status = client.fine_tuning.jobs.retrieve(fine_tune_job.id).status\n",
    "    print(f\"Updated status: {status}\")\n",
    "\n",
    "# Once completed, retrieve the fine-tuned model name\n",
    "if status == \"succeeded\":\n",
    "    fine_tuned_model = client.fine_tuning.jobs.retrieve(fine_tune_job.id).fine_tuned_model\n",
    "    print(f\"Fine-tuned model name: {fine_tuned_model}\")\n",
    "else:\n",
    "    print(\"Fine-tuning job failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128431c7-d88c-4f5d-84bb-afd97dfbd500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve detailed job information\n",
    "fine_tune_details = client.fine_tuning.jobs.retrieve(fine_tune_job.id)\n",
    "\n",
    "# Print detailed error if the job failed\n",
    "if fine_tune_details.status == \"failed\":\n",
    "    error_info = fine_tune_details.error\n",
    "    print(f\"❌ Fine-tuning failed.\\nError message: {error_info.message}\\nError code: {error_info.code}\")\n",
    "else:\n",
    "    print(f\"✅ Fine-tuning status: {fine_tune_details.status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5c58794-0cfc-4e01-a9cd-d628fe1ea30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'concept': 'adverse selection and policy limits', 'question': 'Analyze how policy limits in insurance contracts serve as a mechanism to mitigate adverse selection problems, and evaluate their effectiveness compared to other mitigation strategies.', 'answer': 'Policy limits mitigate adverse selection by capping insurer exposure, thus reducing the asymmetric information advantage of high-risk policyholders. Unlike other mitigation strategies (risk classification, experience rating, deductibles), policy limits specifically address tail risk rather than frequency risk. Their effectiveness lies in aligning interests for catastrophic events where information asymmetries are most severe. However, they create incomplete risk transfer and potential underinsurance for legitimately high-risk individuals. Compared to screening mechanisms like medical underwriting or risk-based pricing, policy limits are less effective at separating risk types but more effective at limiting maximum exposure. The optimal approach combines policy limits with other techniques to create a multi-layered defense against adverse selection.'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_path = \"../data/embeddings/questions/qa_bank_master.jsonl\"\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as infile:\n",
    "    for i, line in enumerate(infile):\n",
    "        entry = json.loads(line.strip())\n",
    "        print(entry)  # prints the structure\n",
    "        if i == 0: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26f47729-092d-4bbc-9ab6-d94ab7841db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Converted file saved as: ../data/embeddings/questions/qa_bank_master_openai.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_path = \"../data/embeddings/questions/qa_bank_master.jsonl\"\n",
    "output_path = \"../data/embeddings/questions/qa_bank_master_openai.jsonl\"\n",
    "\n",
    "system_instruction = \"You are an expert financial assistant.\"\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as infile, open(output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for line in infile:\n",
    "        entry = json.loads(line.strip())\n",
    "        new_entry = {\n",
    "            'messages': [\n",
    "                {\"role\": \"system\", \"content\": system_instruction},\n",
    "                {\"role\": \"user\", \"content\": entry[\"question\"]},\n",
    "                {\"role\": \"assistant\", \"content\": entry[\"answer\"]}\n",
    "            ]\n",
    "        }\n",
    "        outfile.write(json.dumps(new_entry) + \"\\n\")\n",
    "\n",
    "print(f\"✅ Converted file saved as: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b69e8d4-eb30-484a-b535-b4c7fa803a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded file ID: file-PvyL95xbiqtfQg6B1xeBVR\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load your API key from the .env file\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Upload the transformed training file\n",
    "training_file_path = \"../data/embeddings/questions/qa_bank_master_openai.jsonl\"\n",
    "\n",
    "with open(training_file_path, \"rb\") as f:\n",
    "    training_file = client.files.create(file=f, purpose=\"fine-tune\")\n",
    "\n",
    "print(f\"✅ Uploaded file ID: {training_file.id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "caba2d5d-30a8-4b84-8a72-7b779d0b1d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Fine-tuning job created: ftjob-4B8b05aHOHDn6kqCncOnR9Lg\n"
     ]
    }
   ],
   "source": [
    "# Start fine-tuning job (default hyperparameters recommended)\n",
    "fine_tune_job = client.fine_tuning.jobs.create(\n",
    "    training_file=training_file.id,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")\n",
    "\n",
    "print(f\"🚀 Fine-tuning job created: {fine_tune_job.id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1ef90b8-e75b-470a-8fd5-76cd06b11f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current status: validating_files\n",
      "Updated status: validating_files\n",
      "Updated status: validating_files\n",
      "Updated status: validating_files\n",
      "Updated status: validating_files\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: running\n",
      "Updated status: succeeded\n",
      "🎉 Fine-tuned model name: ft:gpt-3.5-turbo-0125:fmai::BNlDwJo3\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Check initial status\n",
    "status = client.fine_tuning.jobs.retrieve(fine_tune_job.id).status\n",
    "print(f\"Current status: {status}\")\n",
    "\n",
    "# Wait and periodically check until the job completes\n",
    "while status not in [\"succeeded\", \"failed\"]:\n",
    "    time.sleep(10)  # Check every 10 seconds\n",
    "    status = client.fine_tuning.jobs.retrieve(fine_tune_job.id).status\n",
    "    print(f\"Updated status: {status}\")\n",
    "\n",
    "# Once completed, retrieve the fine-tuned model ID\n",
    "if status == \"succeeded\":\n",
    "    fine_tuned_model = client.fine_tuning.jobs.retrieve(fine_tune_job.id).fine_tuned_model\n",
    "    print(f\"🎉 Fine-tuned model name: {fine_tuned_model}\")\n",
    "else:\n",
    "    print(\"❌ Fine-tuning job failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee11fd30-2428-4b75-9883-ac589fcbdb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Response from fine-tuned model:\n",
      "Policy limits mitigate adverse selection through two mechanisms: moral hazard management and risk diversification. Insurers use policy limits to reduce moral hazard by aligning coverage with policyholders' willingness to prevent losses. This encourages voluntary risk management among policyholders who understand they bear partial responsibility beyond policy limits. Additionally, policy limits force higher-risk individuals to self-select into excess or umbrella coverage, where underwriters can more accurately price their risk through specialized rating structures. From a portfolio perspective, policy limits also create natural diversification as insurers accumulate policyholders with different aggregate risk profiles. The result is a risk pool where potential large losses are distributed among many policyholders, with only moderately correlated risk exposures—compounding the risk mitigation impact of policy limits beyond individual underwriting characteristics.\n"
     ]
    }
   ],
   "source": [
    "# Access to my own fine tuned AI\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key from your .env file\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Your fine-tuned model ID\n",
    "fine_tuned_model = \"ft:gpt-3.5-turbo-0125:fmai::BNlDwJo3\"\n",
    "\n",
    "# Test your fine-tuned model\n",
    "response = client.chat.completions.create(\n",
    "    model=fine_tuned_model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert financial assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Analyze how policy limits in insurance mitigate adverse selection.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"🚀 Response from fine-tuned model:\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20e9cfc9-936d-442e-bf2f-d1114179c3c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Response from fine-tuned model:\n",
      "I have an advanced understanding of order flow theory, which could be the basis for a highly profitable trading strategy. The strategy would involve identifying significant imbalances between buying and selling pressure at critical support and resistance levels. This would require precise data on order executions, volume profile analysis, time and sales data, and market heat maps to anticipate potential price movements with exceptional accuracy. The strategy's core would revolve around: (1) Identifying market absorption levels where large orders are filled, (2) Recognizing when order flow diverges from price movement, (3) Implementing microstructure-based risk management techniques including stop runs and iceberg detection, and (4) Executing with very low latency to capitalize on fleeting opportunities created by unidirectional order flow. The strategy could be further enhanced through correlation analysis with other instruments showing leading or lagging order flow signals and through machine-learning techniques for pattern recognition within the order flow data. When properly executed, this strategy could yield win rates significantly above 60% with favorable risk-to-reward profiles above 1:3.\n"
     ]
    }
   ],
   "source": [
    "# Access to my own fine tuned AI\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key from your .env file\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Your fine-tuned model ID\n",
    "fine_tuned_model = \"ft:gpt-3.5-turbo-0125:fmai::BNlDwJo3\"\n",
    "\n",
    "# Test your fine-tuned model\n",
    "response = client.chat.completions.create(\n",
    "    model=fine_tuned_model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert financial assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"How much do you know about orderflow theory? could you create a profitable strategy around it with your knowledge\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"🚀 Response from fine-tuned model:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b68a035-bd29-436f-b7bc-f122ae6dc6ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
