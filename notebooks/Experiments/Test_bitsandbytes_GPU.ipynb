{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44fd21f5-c2df-44c1-8c19-cdc77d02c7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:291: UserWarning: \n",
      "NVIDIA GeForce RTX 5090 with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_70 sm_75 sm_80 sm_86 sm_90.\n",
      "If you want to use the NVIDIA GeForce RTX 5090 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bitsandbytes version: 0.45.5\n",
      "Error with bitsandbytes CUDA kernel explicitly: CUDA error: no kernel image is available for execution on the device\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch  # <--- Explicitly add this line\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "print(\"Bitsandbytes version:\", bnb.__version__)\n",
    "\n",
    "# Explicitly test bitsandbytes CUDA functionality\n",
    "try:\n",
    "    a = torch.randn((1024, 1024), device='cuda')\n",
    "    b = torch.randn((1024, 1024), device='cuda')\n",
    "\n",
    "    result = bnb.matmul(a, b)\n",
    "    print(\"bitsandbytes CUDA kernel explicitly working correctly!\")\n",
    "except Exception as e:\n",
    "    print(\"Error with bitsandbytes CUDA kernel explicitly:\", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fae9dd6f-c82a-47e9-9d3a-e4059e2ec42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bitsandbytes version: 0.45.5\n",
      "Error with bitsandbytes CUDA kernel explicitly: CUDA error: no kernel image is available for execution on the device\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import bitsandbytes as bnb\n",
    "\n",
    "print(\"Bitsandbytes version:\", bnb.__version__)\n",
    "\n",
    "# Explicitly test bitsandbytes CUDA functionality\n",
    "try:\n",
    "    a = torch.randn((1024, 1024), device='cuda')\n",
    "    b = torch.randn((1024, 1024), device='cuda')\n",
    "    \n",
    "    result = bnb.matmul(a, b)\n",
    "    print(\"bitsandbytes CUDA kernel explicitly working correctly!\")\n",
    "except Exception as e:\n",
    "    print(\"Error with bitsandbytes CUDA kernel explicitly:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb1fea77-176c-494f-8d01-5b8f040c228f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error explicitly during advanced GPU model loading test: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2.\n",
      "401 Client Error. (Request ID: Root=1-680566ad-4b9897154bf7ec0832403f51;23b70363-edd5-4100-83dc-a4bb532ad4f3)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/resolve/main/config.json.\n",
      "Access to model mistralai/Mistral-7B-Instruct-v0.2 is restricted. You must have access to it and be authenticated to access it. Please log in.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Bitsandbytes quantization configuration explicitly\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_config\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    print(\"Model explicitly loaded successfully with GPU and quantization.\")\n",
    "except Exception as e:\n",
    "    print(\"Error explicitly during advanced GPU model loading test:\", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "848cf43a-2f20-4731-ad1f-ef5308d9e77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running explicit GPU stress test with tensor size 8192...\n",
      "Error explicitly during GPU stress test: CUDA error: no kernel image is available for execution on the device\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def gpu_stress_test(size=10000):\n",
    "    print(f\"Running explicit GPU stress test with tensor size {size}...\")\n",
    "\n",
    "    try:\n",
    "        a = torch.randn((size, size), device='cuda')\n",
    "        b = torch.randn((size, size), device='cuda')\n",
    "\n",
    "        c = torch.matmul(a, b)\n",
    "        print(\"GPU explicitly computed matrix multiplication correctly.\")\n",
    "        \n",
    "        del a, b, c\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"GPU cache explicitly cleared after test.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error explicitly during GPU stress test:\", e)\n",
    "\n",
    "gpu_stress_test(size=8192)  # Adjust size if explicit memory limits encountered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01832715-998d-4afd-90af-6a36c617ad21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES: Not explicitly set\n",
      "LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "BNB_CUDA_VERSION: Not explicitly set\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check environment variables explicitly relevant for CUDA\n",
    "cuda_visible_devices = os.getenv(\"CUDA_VISIBLE_DEVICES\", \"Not explicitly set\")\n",
    "ld_library_path = os.getenv(\"LD_LIBRARY_PATH\", \"Not explicitly set\")\n",
    "bnb_cuda_version = os.getenv(\"BNB_CUDA_VERSION\", \"Not explicitly set\")\n",
    "\n",
    "print(f\"CUDA_VISIBLE_DEVICES: {cuda_visible_devices}\")\n",
    "print(f\"LD_LIBRARY_PATH: {ld_library_path}\")\n",
    "print(f\"BNB_CUDA_VERSION: {bnb_cuda_version}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cfc0a5-6152-4261-96ab-aaeb1f28fec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638dada9-b56c-4276-a9aa-e3f844ed0bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
