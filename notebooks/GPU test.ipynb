{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a96a9a82-0a27-4c1e-91d4-03e3664ed3d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.8.0.dev20250421+cu128\n",
      "CUDA available: True\n",
      "Device name: NVIDIA GeForce RTX 5090\n",
      "Compute capability: (12, 0)\n",
      "Explicit GPU tensor calculation successful: 1249.7547607421875\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Explicit GPU Check\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Compute capability: {torch.cuda.get_device_capability(0)}\")\n",
    "\n",
    "# Explicit GPU tensor operation (matrix multiplication test)\n",
    "x = torch.rand(5000, 5000, device='cuda')\n",
    "y = torch.rand(5000, 5000, device='cuda')\n",
    "\n",
    "result = (x @ y).mean()\n",
    "print(f\"Explicit GPU tensor calculation successful: {result}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "086b6821-7098-43c7-9e26-243312bb1cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: Hello, GPU is explicitly working:GMI is working.\n",
      "\n",
      "The GPU is working.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# explicitly using GPT-2 small for fast testing\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to('cuda')\n",
    "\n",
    "inputs = tokenizer(\"Hello, GPU is explicitly working:GMI\", return_tensors=\"pt\").to('cuda')\n",
    "outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "\n",
    "print(\"Output:\", tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28561164-78cc-41d8-9f55-b7720f18fb6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5240ff-a764-4048-aac2-2a43bf808985",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
