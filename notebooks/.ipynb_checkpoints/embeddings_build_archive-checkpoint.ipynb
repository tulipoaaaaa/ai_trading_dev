{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd34d12-e971-4e40-a720-857edd0ac74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss, json, pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"/workspace/data/embeddings\")\n",
    "index = faiss.read_index(str(DATA_DIR / \"faiss_direct.index\"))\n",
    "\n",
    "meta = pd.read_csv(DATA_DIR / \"faiss_chunks_metadata_direct.csv\")\n",
    "assert index.ntotal == len(meta), \"Mismatch between vectors and metadata!\"\n",
    "DIM = index.d  # should be 384 for all‚ÄëMiniLM‚ÄëL6‚Äëv2\n",
    "print(f\"Vectors: {index.ntotal:,} | Dim: {DIM}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1295fc7f-1aeb-4169-9b1b-536510b143d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {\n",
    "    \"vector_count\": index.ntotal,\n",
    "    \"dimension\": DIM,\n",
    "    \"mean_norm\": float(np.mean(np.linalg.norm(index.reconstruct_n(0, index.ntotal), axis=1))),\n",
    "}\n",
    "\n",
    "# Optional: include duplication rate only if chunk_sha256 exists\n",
    "if \"chunk_sha256\" in meta.columns:\n",
    "    stats[\"duplication_rate\"] = float(meta[\"chunk_sha256\"].duplicated().mean())\n",
    "else:\n",
    "    stats[\"duplication_rate\"] = None  # Mark it missing\n",
    "\n",
    "# Save JSON\n",
    "with open(DATA_DIR / \"faiss_stats.json\", \"w\") as fp:\n",
    "    json.dump(stats, fp, indent=2)\n",
    "\n",
    "stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dfc175-34b3-4523-8550-e40946810d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load all vectors from FAISS\n",
    "X = np.stack([index.reconstruct(i) for i in range(index.ntotal)])\n",
    "\n",
    "# Project to 2D\n",
    "print(\"Running t-SNE...\")\n",
    "X_2d = TSNE(n_components=2, perplexity=50, init='pca', random_state=42).fit_transform(X)\n",
    "\n",
    "# Cluster into k groups (can tune this number)\n",
    "k = 10\n",
    "print(f\"Running k-means with k={k}...\")\n",
    "km = KMeans(n_clusters=k, random_state=42).fit(X)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(X_2d[:,0], X_2d[:,1], c=km.labels_, cmap='tab10', s=5, alpha=0.7)\n",
    "plt.title(\"Knowledge Cluster Map (t-SNE projection)\")\n",
    "plt.xlabel(\"t-SNE Dim 1\")\n",
    "plt.ylabel(\"t-SNE Dim 2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7161ef-d603-46cd-82ba-b5163af97a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# Load the same model as your embedding pipeline\n",
    "encoder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Define test queries\n",
    "queries = [\n",
    "    \"What is order flow imbalance?\",\n",
    "    \"Explain the Kelly criterion.\",\n",
    "    \"What causes slippage in trade execution?\",\n",
    "    \"What is market microstructure?\",\n",
    "    \"Define adverse selection in trading.\",\n",
    "]\n",
    "\n",
    "# Top-k neighbors to retrieve\n",
    "k = 3\n",
    "\n",
    "# Run semantic search\n",
    "for q in queries:\n",
    "    print(\"=\"*120)\n",
    "    print(f\"üîç Query: {q}\")\n",
    "    q_embed = encoder.encode([q])\n",
    "    D, I = index.search(q_embed, k)\n",
    "\n",
    "    for i, idx in enumerate(I[0]):\n",
    "        row = meta.iloc[idx]\n",
    "        print(f\"\\n‚Üí Result {i+1} (score: {D[0][i]:.4f}):\")\n",
    "        print(\"-\"*100)\n",
    "        print(f\"[Source: {row.get('source', 'N/A')} | Page: {row.get('page', 'N/A')}]\")\n",
    "        print(row.get(\"text_preview\", \"[No content]\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860a7f23-12f7-42ec-b4d2-da8c2ee67261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "import pandas as pd\n",
    "\n",
    "# Number of clusters\n",
    "k = 10\n",
    "\n",
    "# Re-run k-means to get centroids (if needed)\n",
    "km = KMeans(n_clusters=k, random_state=42).fit(X)\n",
    "\n",
    "# Find medoid of each cluster (closest chunk to cluster center)\n",
    "medoid_idxs, _ = pairwise_distances_argmin_min(km.cluster_centers_, X)\n",
    "\n",
    "# Print results\n",
    "cluster_labels = {}\n",
    "\n",
    "for i, medoid_idx in enumerate(medoid_idxs):\n",
    "    row = meta.iloc[medoid_idx]\n",
    "    text = row.get(\"text_preview\", \"[No preview]\")\n",
    "    source = row.get(\"source\", \"Unknown\")\n",
    "    page = row.get(\"page\", \"?\")\n",
    "\n",
    "    print(\"=\"*100)\n",
    "    print(f\"üß† Cluster {i} ‚Äî Suggested Medoid (chunk #{medoid_idx})\")\n",
    "    print(f\"[Source: {source} | Page: {page}]\")\n",
    "    print(f\"Preview: {text[:500]}...\\n\")\n",
    "\n",
    "    # Optional: manually name it now\n",
    "    cluster_labels[i] = f\"Cluster {i} ‚Äî [Name me based on above]\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae33156-73e8-4ff6-ae66-1ad2cf5a03ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = {\n",
    "    0: \"Crypto Protocols & Architecture\",\n",
    "    1: \"Technical Analysis ‚Äì Risk Framing\",\n",
    "    2: \"Chart Patterns & Price Extremes\",\n",
    "    3: \"Quantitative Finance & Simons\",\n",
    "    4: \"Options Pricing & Volatility Math\",\n",
    "    5: \"Technical Indicators ‚Äì Reliability\",\n",
    "    6: \"Macro Risk Management\",\n",
    "    7: \"Market Liquidity & Order Flow\",\n",
    "    8: \"ML/Stats in Quant Models\",\n",
    "    9: \"Crypto Market Structure & Risks\",\n",
    "}\n",
    "\n",
    "# Save to CSV for inspection or further mapping\n",
    "meta[\"cluster_id\"] = km.labels_\n",
    "meta[\"cluster_label\"] = meta[\"cluster_id\"].map(cluster_labels)\n",
    "meta.to_csv(DATA_DIR / \"metadata_with_clusters.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6674d7a1-42dc-46b7-a6de-f835403776ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Your full concept list (shortened version shown here)\n",
    "concepts = [\n",
    "     # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 1. Market Microstructure ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    \"limit order book\",\n",
    "    \"market depth\",\n",
    "    \"order book imbalance\",\n",
    "    \"order matching engine\",\n",
    "    \"price discovery\",\n",
    "    \"bid-ask spread\",\n",
    "    \"tick size\",\n",
    "    \"quote stuffing\",\n",
    "    \"market impact\",\n",
    "    \"adverse selection\",\n",
    "    \"hidden liquidity\",\n",
    "    \"latency arbitrage\",\n",
    "    \"maker-taker fees\",\n",
    "    \"high-frequency trading\",\n",
    "    \"flash crash dynamics\",\n",
    "    \"order book spoofing\",\n",
    "    \"fill probability\",\n",
    "    \"queue position\",\n",
    "    \"liquidity fragmentation\",\n",
    "    \"trading halts\",\n",
    "    \"short-term auction effects\",\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 2. Order Flow & Execution ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    \"execution algorithms\",\n",
    "    \"TWAP\",\n",
    "    \"VWAP\",\n",
    "    \"implementation shortfall\",\n",
    "    \"slippage\",\n",
    "    \"smart order routing\",\n",
    "    \"parent/child order logic\",\n",
    "    \"iceberg orders\",\n",
    "    \"passive vs aggressive order flow\",\n",
    "    \"cancel-replace ratio\",\n",
    "    \"execution latency\",\n",
    "    \"fill ratio\",\n",
    "    \"trade execution certainty\",\n",
    "    \"trade sequencing\",\n",
    "    \"order flow imbalance\",\n",
    "    \"quote-to-trade ratio\",\n",
    "    \"multi-leg order routing\",\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 3. Technical & Quant Analysis ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    \"moving average crossover\",\n",
    "    \"relative strength index\",\n",
    "    \"macd\",\n",
    "    \"bollinger bands\",\n",
    "    \"fibonacci retracement\",\n",
    "    \"volume profile\",\n",
    "    \"trendlines\",\n",
    "    \"candlestick reversal patterns\",\n",
    "    \"momentum divergence\",\n",
    "    \"price action\",\n",
    "    \"trend following\",\n",
    "    \"mean reversion\",\n",
    "    \"chart pattern recognition\",\n",
    "    \"indicator lag\",\n",
    "    \"supply and demand zones\",\n",
    "    \"volume weighted indicators\",\n",
    "    \"fractals in price data\",\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 4. Portfolio & Risk Management ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    \"value at risk\",\n",
    "    \"conditional value at risk\",\n",
    "    \"maximum drawdown\",\n",
    "    \"risk-adjusted return\",\n",
    "    \"sharpe ratio\",\n",
    "    \"sortino ratio\",\n",
    "    \"omega ratio\",\n",
    "    \"tail risk\",\n",
    "    \"liquidity risk\",\n",
    "    \"credit risk\",\n",
    "    \"counterparty risk\",\n",
    "    \"position sizing\",\n",
    "    \"kelly criterion\",\n",
    "    \"exposure management\",\n",
    "    \"stress testing\",\n",
    "    \"scenario analysis\",\n",
    "    \"margin risk\",\n",
    "    \"portfolio volatility\",\n",
    "    \"correlation matrix\",\n",
    "    \"beta exposure\",\n",
    "    \"concentration risk\",\n",
    "    \"regulatory capital\",\n",
    "    \"drawdown control\",\n",
    "    \"trading halts and kill switches\",\n",
    "    \"value decay over drawdowns\",\n",
    "    \"stop loss engineering\",\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 5. Psychology & Behavioral Risk ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    \"loss aversion\",\n",
    "    \"confirmation bias\",\n",
    "    \"recency bias\",\n",
    "    \"overconfidence bias\",\n",
    "    \"anchoring bias\",\n",
    "    \"emotional trading\",\n",
    "    \"overtrading\",\n",
    "    \"revenge trading\",\n",
    "    \"fear and greed cycles\",\n",
    "    \"trader discipline\",\n",
    "    \"decision fatigue\",\n",
    "    \"mental stop loss\",\n",
    "    \"self-sabotage in trading\",\n",
    "    \"cognitive dissonance\",\n",
    "    \"regret minimization\",\n",
    "    \"risk appetite profiling\",\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 6. Crypto Market Structure ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    \"on-chain liquidity\",\n",
    "    \"cex vs dex\",\n",
    "    \"impermanent loss\",\n",
    "    \"oracle latency\",\n",
    "    \"stablecoin depegging risk\",\n",
    "    \"MEV (maximal extractable value)\",\n",
    "    \"rug pull\",\n",
    "    \"slashing risk\",\n",
    "    \"smart contract risk\",\n",
    "    \"liquidity mining\",\n",
    "    \"staking derivatives\",\n",
    "    \"yield farming\",\n",
    "    \"token vesting cliffs\",\n",
    "    \"governance attacks\",\n",
    "    \"cross-chain bridge risk\",\n",
    "    \"custody vs self custody\",\n",
    "    \"flash loan attacks\",\n",
    "    \"TVL manipulation\",\n",
    "    \"gas fee volatility\",\n",
    "    \"proof of reserves\",\n",
    "    \"validator concentration risk\",\n",
    "    \"exchange insolvency risk\",\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 7. Macroeconomics & Liquidity ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    \"interest rate risk\",\n",
    "    \"yield curve inversion\",\n",
    "    \"inflation targeting\",\n",
    "    \"quantitative tightening\",\n",
    "    \"quantitative easing\",\n",
    "    \"federal funds rate\",\n",
    "    \"liquidity trap\",\n",
    "    \"repo operations\",\n",
    "    \"reverse repo facility\",\n",
    "    \"dollar liquidity cycles\",\n",
    "    \"cross-currency basis swap\",\n",
    "    \"USD collateral tightening\",\n",
    "    \"global risk-on risk-off\",\n",
    "    \"monetary policy divergence\",\n",
    "    \"macro liquidity regime shifts\",\n",
    "    \"CB balance sheet expansion\",\n",
    "    \"eurodollar system\",\n",
    "    \"shadow banking system\",\n",
    "    \"global USD shortage\",\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 8. Machine Learning & Modeling ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    \"overfitting\",\n",
    "    \"cross-validation\",\n",
    "    \"feature importance\",\n",
    "    \"hyperparameter tuning\",\n",
    "    \"gradient boosting trees\",\n",
    "    \"long short-term memory networks\",\n",
    "    \"temporal convolutional networks\",\n",
    "    \"transformer models for trading\",\n",
    "    \"walk forward validation\",\n",
    "    \"sliding window retraining\",\n",
    "    \"online learning\",\n",
    "    \"non-stationary time series\",\n",
    "    \"target leakage\",\n",
    "    \"ensemble methods\",\n",
    "    \"reinforcement learning for trade execution\",\n",
    "    \"meta-labeling\",\n",
    "    \"probabilistic confidence calibration\",\n",
    "    \"covariate shift detection\",\n",
    "    \"signal decay tracking\",\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 9. Derivatives & Options ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    \"option greeks\",\n",
    "    \"gamma exposure\",\n",
    "    \"delta hedging\",\n",
    "    \"theta decay\",\n",
    "    \"vega compression\",\n",
    "    \"skew arbitrage\",\n",
    "    \"implied vs realized volatility\",\n",
    "    \"volatility surface modeling\",\n",
    "    \"straddle\",\n",
    "    \"strangle\",\n",
    "    \"covered call\",\n",
    "    \"iron condor\",\n",
    "    \"volatility term structure\",\n",
    "    \"option open interest dynamics\",\n",
    "    \"tail hedging\",\n",
    "    \"volatility crush\",\n",
    "    \"synthetic replication\",\n",
    "    \"OTM skew behavior\",\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 10. Regulation, Fraud, Infrastructure Risk ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    \"insider trading\",\n",
    "    \"wash trading\",\n",
    "    \"pump and dump\",\n",
    "    \"front-running bots\",\n",
    "    \"exchange outage risk\",\n",
    "    \"flash crash regulation\",\n",
    "    \"spoofing and layering\",\n",
    "    \"market manipulation\",\n",
    "    \"dark pool reporting rules\",\n",
    "    \"KYC AML enforcement\",\n",
    "    \"regulatory arbitrage\",\n",
    "    \"jurisdictional risk\",\n",
    "    \"market surveillance\",\n",
    "    \"algo trading compliance\",\n",
    "    \"post-trade transparency\",\n",
    "    \"circuit breaker logic\",\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 11. Meta-Reasoning, Evaluation & Agent Tools ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    \"reasoning under uncertainty\",\n",
    "    \"meta-cognition in strategy switching\",\n",
    "    \"confidence scoring\",\n",
    "    \"backtest overfitting detection\",\n",
    "    \"real vs simulated performance tracking\",\n",
    "    \"regime detection\",\n",
    "    \"behavioral edge extraction\",\n",
    "    \"reward shaping for AI agents\",\n",
    "    \"agent burn-in period\",\n",
    "    \"decision tree vs ensemble path tracing\",\n",
    "    \"multi-agent coordination risk\",\n",
    "    \"agent self-correction heuristics\"\n",
    "]\n",
    "\n",
    "# Convert the list into a DataFrame (like a spreadsheet in memory)\n",
    "concepts_df = pd.DataFrame({\"concept\": concepts})\n",
    "\n",
    "# Save that DataFrame to a .csv file in your /workspace/data folder\n",
    "concepts_df.to_csv(DATA_DIR / \"concept_list.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Your concept list has been saved to /workspace/data/concept_list.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b7b425-b4c4-40e8-ad17-cb42881da28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir(DATA_DIR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cc1d04-a098-4250-a42b-f99c1a4a8eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts_df = pd.read_csv(DATA_DIR / \"concept_list.csv\")\n",
    "print(concepts_df.head())\n",
    "print(f\"‚úÖ Loaded {len(concepts_df)} concepts from CSV\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513a9ef4-7f9e-42e7-85b7-ff6e0d9d8938",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load concept list from the saved CSV\n",
    "concepts_df = pd.read_csv(DATA_DIR / \"concept_list.csv\")\n",
    "concepts = concepts_df[\"concept\"].tolist()\n",
    "\n",
    "# Load the encoder model (same as used in embeddings)\n",
    "encoder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Parameters\n",
    "threshold = 0.55  # Lower = stricter match\n",
    "coverage = []\n",
    "\n",
    "print(f\"üîç Running coverage check on {len(concepts)} concepts...\")\n",
    "\n",
    "for concept in concepts:\n",
    "    q_embed = encoder.encode([concept])\n",
    "    D, I = index.search(q_embed, 1)\n",
    "    dist = D[0][0]\n",
    "    idx = I[0][0]\n",
    "    row = meta.iloc[idx]\n",
    "\n",
    "    is_covered = dist <= threshold\n",
    "    coverage.append({\n",
    "        \"concept\": concept,\n",
    "        \"covered\": is_covered,\n",
    "        \"distance\": dist,\n",
    "        \"best_chunk\": row.get(\"text_preview\", \"[No preview]\"),\n",
    "        \"source\": row.get(\"source\", \"Unknown\"),\n",
    "        \"page\": row.get(\"page\", \"?\")\n",
    "    })\n",
    "\n",
    "# Save results to CSV\n",
    "coverage_df = pd.DataFrame(coverage)\n",
    "coverage_df.to_csv(DATA_DIR / \"concept_coverage_results.csv\", index=False)\n",
    "\n",
    "# Show top 10 best matches\n",
    "print(\"‚úÖ Coverage check complete. Showing top 10 strongest matches:\\n\")\n",
    "coverage_df.sort_values(\"distance\").head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebc3df3-e186-4f0f-bc9a-0e710c3dd707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all concepts not confidently covered\n",
    "missing_df = coverage_df[coverage_df[\"covered\"] == False].sort_values(\"distance\")\n",
    "missing_df.to_csv(DATA_DIR / \"concepts_missing_or_weak.csv\", index=False)\n",
    "\n",
    "print(\"üìÑ Exported missing/weak concepts to 'concepts_missing_or_weak.csv'\")\n",
    "missing_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc57f8d-b8f9-4a7c-87d0-1f7713a47df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "DATA_DIR = Path(\"/workspace/data/embeddings\")   # same path you used earlier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aee0522-b774-4ba1-9299-f5932e7d5f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib\n",
    "priority_dir = Path(\"/workspace/priority_pdfs\")   # your real path\n",
    "print(\"Files:\", os.listdir(priority_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de226ef-4b31-465e-9313-cf6ca1b12ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "#  High‚ÄëResolution Re‚ÄëEmbedding of Priority PDFs\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "from pathlib import Path\n",
    "import os, hashlib\n",
    "import fitz                               # PyMuPDF\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ----- paths -----\n",
    "# main embeddings folder (same as earlier)\n",
    "DATA_DIR = Path(\"/workspace/data/embeddings\")\n",
    "# folder where the uploaded PDFs live\n",
    "priority_dir = Path(\"/workspace/priority_pdfs\")\n",
    "\n",
    "# ----- discover all PDFs automatically -----\n",
    "pdf_files = sorted([p for p in priority_dir.glob(\"*.pdf\")])\n",
    "print(\"üìö PDFs found for re‚Äëembedding:\")\n",
    "for p in pdf_files:\n",
    "    print(\"  ‚Ä¢\", p.name)\n",
    "\n",
    "# ----- parameters -----\n",
    "chunk_size = 350        # smaller chunks = higher resolution\n",
    "model      = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectors    = []\n",
    "metadata   = []\n",
    "\n",
    "# ----- process each PDF -----\n",
    "for pdf_path in pdf_files:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    print(f\"\\nüìñ Processing: {pdf_path.name} ({len(doc)} pages)\")\n",
    "\n",
    "    for page_num in range(len(doc)):\n",
    "        text = doc[page_num].get_text()\n",
    "        if not text or len(text) < 50:\n",
    "            continue\n",
    "\n",
    "        # chunk the page text\n",
    "        for i in range(0, len(text), chunk_size):\n",
    "            chunk = text[i : i + chunk_size].strip()\n",
    "            if len(chunk) < 100:\n",
    "                continue\n",
    "\n",
    "            vec = model.encode(chunk).astype(np.float32)\n",
    "            vectors.append(vec)\n",
    "\n",
    "            metadata.append({\n",
    "                \"chunk_id\"      : hashlib.sha256(chunk.encode()).hexdigest(),\n",
    "                \"text_preview\"  : chunk[:400],\n",
    "                \"source\"        : pdf_path.name,\n",
    "                \"page\"          : page_num + 1,\n",
    "                \"is_priority\"   : True\n",
    "            })\n",
    "\n",
    "# ----- build & save FAISS index -----\n",
    "if not vectors:\n",
    "    raise ValueError(\"No valid chunks extracted. Check PDF contents.\")\n",
    "\n",
    "dimension = len(vectors[0])\n",
    "index     = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.vstack(vectors))\n",
    "\n",
    "faiss_path      = DATA_DIR / \"priority_faiss.index\"\n",
    "metadata_path   = DATA_DIR / \"priority_metadata.csv\"\n",
    "\n",
    "faiss.write_index(index, str(faiss_path))\n",
    "pd.DataFrame(metadata).to_csv(metadata_path, index=False)\n",
    "\n",
    "print(\"\\n‚úÖ High‚Äëresolution index & metadata saved:\")\n",
    "print(\"   ‚Ä¢\", faiss_path)\n",
    "print(\"   ‚Ä¢\", metadata_path)\n",
    "print(\"   ‚Ä¢ Chunks embedded:\", len(vectors))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a7164a-8b35-4d4b-93ef-51a3f64f7c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss, pandas as pd\n",
    "\n",
    "encoder         = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "main_index      = faiss.read_index(str(DATA_DIR / \"faiss_direct.index\"))\n",
    "priority_index  = faiss.read_index(str(DATA_DIR / \"priority_faiss.index\"))\n",
    "\n",
    "meta_main       = pd.read_csv(DATA_DIR / \"faiss_chunks_metadata_direct.csv\")\n",
    "meta_priority   = pd.read_csv(DATA_DIR / \"priority_metadata.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab9aff1-1e3c-4d39-bcc8-6d62517950eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, k=5, boost=0.10):\n",
    "    q_vec = encoder.encode([query])\n",
    "    \n",
    "    # 1) priority search\n",
    "    Dp, Ip = priority_index.search(q_vec, k)\n",
    "    results = []\n",
    "    for d, i in zip(Dp[0], Ip[0]):\n",
    "        score = (1 - d) + boost           # give priority chunks extra weight\n",
    "        row   = meta_priority.iloc[i]\n",
    "        results.append((\"priority\", score, row))\n",
    "    \n",
    "    # 2) main search (fill remaining slots)\n",
    "    if len(results) < k:\n",
    "        Dm, Im = main_index.search(q_vec, k - len(results))\n",
    "        for d, i in zip(Dm[0], Im[0]):\n",
    "            score = (1 - d)               # normal score\n",
    "            row   = meta_main.iloc[i]\n",
    "            results.append((\"main\", score, row))\n",
    "    \n",
    "    # sort by final score descending\n",
    "    results.sort(key=lambda x: -x[1])\n",
    "    return results[:k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707628a9-d1fe-45d6-816a-849b6d07833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = semantic_search(\"explain liquidity risk\", k=5, boost=0.10)\n",
    "\n",
    "for src, score, row in hits:\n",
    "    print(f\"[{src}] {score:.3f} | {row['source']} p.{row['page']}\")\n",
    "    print(\"   \", row['text_preview'][:140], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cc72e9-ec9e-4084-9f1d-3dda10f7336c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "#  Priority‚ÄëFirst Concept Coverage Scan\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import faiss, numpy as np\n",
    "\n",
    "# 1.  Load concept list\n",
    "concepts     = pd.read_csv(DATA_DIR / \"concept_list.csv\")[\"concept\"].tolist()\n",
    "\n",
    "# 2.  Load indices & metadata\n",
    "encoder         = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "priority_index  = faiss.read_index(str(DATA_DIR / \"priority_faiss.index\"))\n",
    "main_index      = faiss.read_index(str(DATA_DIR / \"faiss_direct.index\"))\n",
    "meta_priority   = pd.read_csv(DATA_DIR / \"priority_metadata.csv\")\n",
    "meta_main       = pd.read_csv(DATA_DIR / \"faiss_chunks_metadata_direct.csv\")\n",
    "\n",
    "def hybrid_search(vec, k=1, boost=0.10):\n",
    "    # priority first\n",
    "    Dp, Ip = priority_index.search(vec, k)\n",
    "    best_d, best_i, best_src = Dp[0][0], Ip[0][0], \"priority\"\n",
    "    best_score = (1 - best_d) + boost\n",
    "    \n",
    "    # main fallback\n",
    "    Dm, Im = main_index.search(vec, 1)\n",
    "    alt_d, alt_i = Dm[0][0], Im[0][0]\n",
    "    alt_score    = (1 - alt_d)\n",
    "    \n",
    "    # choose highest score\n",
    "    if alt_score > best_score:\n",
    "        return \"main\", alt_d, alt_i\n",
    "    else:\n",
    "        return best_src, best_d, best_i\n",
    "\n",
    "results = []\n",
    "threshold = 0.65   # keep same threshold\n",
    "\n",
    "for concept in concepts:\n",
    "    q_vec = encoder.encode([concept])\n",
    "    src, dist, idx = hybrid_search(q_vec, k=1, boost=0.10)\n",
    "    \n",
    "    row = meta_priority.iloc[idx] if src == \"priority\" else meta_main.iloc[idx]\n",
    "    covered = dist <= threshold\n",
    "    \n",
    "    results.append({\n",
    "        \"concept\"   : concept,\n",
    "        \"covered\"   : covered,\n",
    "        \"distance\"  : dist,\n",
    "        \"source\"    : row[\"source\"],\n",
    "        \"page\"      : row[\"page\"],\n",
    "        \"priority?\" : src == \"priority\"\n",
    "    })\n",
    "\n",
    "coverage_df = pd.DataFrame(results)\n",
    "coverage_df.to_csv(DATA_DIR / \"concept_coverage_priority_first.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Scan complete.  Missing / weak concepts:\")\n",
    "missing = coverage_df[coverage_df[\"covered\"] == False].sort_values(\"distance\")\n",
    "display(missing.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d9355fd-01aa-46f3-8097-a108c44e9d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Existing chunks: 8609\n",
      "üîç Scanning Candlestick Trading Technique- Trade Trend Reversal with Pin -- Avinash Mudaliar -- 2017.pdf (17 pages)\n",
      "üîç Scanning Optimal trading strategies - quantitative approaches for -- Robert Kissell; Morton Glantz; Roberto Malamut -- 1st, First Edition, PS, 2003.pdf (408 pages)\n",
      "üîç Scanning Encyclopedia of Candlestick Charts (Wiley Trading) -- Thomas N_ Bulkowski, Thomas N_ Bulkowski -- Wiley trading series, Hoboken, N_J, New Jersey, 2008.pdf (966 pages)\n",
      "üîç Scanning Advances in Financial Machine Learning by LoÃÅpez de Prado, Marcos -- 2018 .pdf (393 pages)\n",
      "üîç Scanning Clearing, settlement, and custody -- David Loader -- 3, 2019 -- Butterworth-Heinemann, an imprint of Elsevier.pdf (331 pages)\n",
      "üîç Scanning Mastering blockchain - unlocking the power of -- Lorne Lantz, Daniel Cawrey -- 1, 2020-12-08 -- O'Reilly Media, Incorporated; O'Reilly Media.pdf (0 pages)\n",
      "üîç Scanning The structure of clearing and settlement -- David Loader (Auth_) -- Clearing, Settlement and Custody, 2, 2014 -- Elsevier Science & Technology Books.pdf (228 pages)\n",
      "üîç Scanning Japanese Candlestick Charting Techniques by Steve Nison, 2009.pdf (298 pages)\n",
      "üîç Scanning The Art and Science of Technical Analysis- Market Structure, -- Adam Grimes -- 2012 -- Wiley.pdf (234 pages)\n",
      "üîç Scanning Trading in the Zone - Master the Market with Confidence by Mark Douglas.pdf (143 pages)\n",
      "üîç Scanning gpt deep dive gaps.pdf (192 pages)\n",
      "üîç Scanning [Algorithmic Trading & DMA- An introduction to direct access trading strategies].pdf (595 pages)\n",
      "üîç Scanning The xVA Challenge- Counterparty Credit Risk, Funding, -- Jon Gregory -- The Wiley Finance Series, 3rd Edition 2015, 2015 -- Wiley & Sons, Limited.pdf (497 pages)\n",
      "üîç Scanning Mastering Ethereum- Building Smart Contracts and DApps -- Andreas M_ Antonopoulos & Gavin Wood Ph_ D_ -- 2018 -- O'Reilly Media.pdf (759 pages)\n",
      "üîç Scanning The Financial Mathematics of Market Liquidity- From Optimal -- Gueant, Olivier -- Chapman & Hall-CRC financial mathematics series.pdf (302 pages)\n",
      "üîç Scanning Money-Making Candlestick Patterns - Backtested for Proven -- Steve Palmquist, Oliver L_ Velez -- Wiley Trading, 90, 1, 2008 -- Wiley.pdf (348 pages)\n",
      "üîç Scanning Counterparty Credit Risk- The new challenge for global -- Jon Gregory -- The Wiley Finance Series, 1, 2010 -- John Wiley & Sons, Incorporated.pdf (450 pages)\n",
      "üîç Scanning Candlestick and pivot point trading triggers - setups for -- John L_ Person -- Wiley trading series, Hoboken, N_J, c2007 -- Wiley & Sons.pdf (351 pages)\n",
      "üîç Scanning risk management and financial institutions.pdf (743 pages)\n",
      "üîç Scanning Mind over markets - power trading with market generated -- James F_ Dalton, Eric T_ Jones, Robert Bevan Dalton -- 2nd edition, May 25, 1999 -- Traders.pdf (350 pages)\n",
      "\n",
      "‚úÖ Added 28777 new chunks from 14 new PDF(s).\n",
      "   Total priority chunks: 37386\n"
     ]
    }
   ],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Incremental re‚Äëembed of priority_pdfs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "from pathlib import Path\n",
    "import fitz, hashlib, faiss, numpy as np, pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. Paths\n",
    "DATA_DIR      = Path(\"/workspace/data/embeddings\")          # main data folder\n",
    "priority_dir  = Path(\"/workspace/priority_pdfs\")            # folder with PDFs\n",
    "index_path    = DATA_DIR / \"priority_faiss.index\"\n",
    "meta_path     = DATA_DIR / \"priority_metadata.csv\"\n",
    "\n",
    "# 2. Load existing index & metadata (if present)\n",
    "if index_path.exists():\n",
    "    priority_index = faiss.read_index(str(index_path))\n",
    "    meta_df        = pd.read_csv(meta_path)\n",
    "    existing_sha   = set(meta_df[\"chunk_id\"])\n",
    "    print(f\"üìö Existing chunks: {len(meta_df)}\")\n",
    "else:\n",
    "    priority_index = None\n",
    "    meta_df        = pd.DataFrame()\n",
    "    existing_sha   = set()\n",
    "    print(\"üìö No existing priority index found ‚Äì fresh start.\")\n",
    "\n",
    "# 3. Parameters\n",
    "chunk_size = 350\n",
    "model      = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 4. Walk through PDFs and collect NEW chunks\n",
    "vectors, new_meta = [], []\n",
    "for pdf_path in priority_dir.glob(\"*.pdf\"):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    print(f\"üîç Scanning {pdf_path.name} ({len(doc)} pages)\")\n",
    "\n",
    "    for page_num in range(len(doc)):\n",
    "        text = doc[page_num].get_text(\"text\")\n",
    "        if not text: \n",
    "            continue\n",
    "        for i in range(0, len(text), chunk_size):\n",
    "            chunk = text[i:i+chunk_size].strip()\n",
    "            if len(chunk) < 100:             # skip tiny fragments\n",
    "                continue\n",
    "            sha = hashlib.sha256(chunk.encode()).hexdigest()\n",
    "            if sha in existing_sha:          # already embedded\n",
    "                continue\n",
    "            vectors.append(model.encode(chunk).astype(np.float32))\n",
    "            new_meta.append({\n",
    "                \"chunk_id\"     : sha,\n",
    "                \"text_preview\" : chunk[:400],\n",
    "                \"source\"       : pdf_path.name,\n",
    "                \"page\"         : page_num + 1,\n",
    "                \"is_priority\"  : True\n",
    "            })\n",
    "\n",
    "# 5. Add new vectors to FAISS and save\n",
    "if vectors:\n",
    "    vecs_np = np.vstack(vectors)\n",
    "    if priority_index is None:                      # first‚Äëtime build\n",
    "        priority_index = faiss.IndexFlatL2(vecs_np.shape[1])\n",
    "    priority_index.add(vecs_np)\n",
    "\n",
    "    faiss.write_index(priority_index, str(index_path))\n",
    "    meta_df = pd.concat([meta_df, pd.DataFrame(new_meta)], ignore_index=True)\n",
    "    meta_df.to_csv(meta_path, index=False)\n",
    "\n",
    "    print(f\"\\n‚úÖ Added {len(vectors)} new chunks \"\n",
    "          f\"from {len(set(m['source'] for m in new_meta))} new PDF(s).\")\n",
    "    print(\"   Total priority chunks:\", len(meta_df))\n",
    "else:\n",
    "    print(\"\\n‚úÖ No new chunks detected. Priority index already up‚Äëto‚Äëdate.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7177844-9ea4-4066-ae82-554b41225203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- reload fresh metadata to match indices ---\n",
    "meta_priority = pd.read_csv(DATA_DIR / \"priority_metadata.csv\")\n",
    "meta_main     = pd.read_csv(DATA_DIR / \"faiss_chunks_metadata_direct.csv\")\n",
    "\n",
    "assert priority_index.ntotal == len(meta_priority), \\\n",
    "       f\"Priority index rows ({priority_index.ntotal}) ‚â† metadata rows ({len(meta_priority)})\"\n",
    "\n",
    "assert main_index.ntotal == len(meta_main), \\\n",
    "       f\"Main index rows ({main_index.ntotal}) ‚â† metadata rows ({len(meta_main)})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9ad7a01-0f54-4e59-a67e-33225cb5617e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Scan complete.  Missing / weak concepts:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concept</th>\n",
       "      <th>covered</th>\n",
       "      <th>distance</th>\n",
       "      <th>source</th>\n",
       "      <th>page</th>\n",
       "      <th>priority?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>price action</td>\n",
       "      <td>False</td>\n",
       "      <td>0.652456</td>\n",
       "      <td>The Art and Science of Technical Analysis- Mar...</td>\n",
       "      <td>36</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>trade execution certainty</td>\n",
       "      <td>False</td>\n",
       "      <td>0.655064</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>scenario analysis</td>\n",
       "      <td>False</td>\n",
       "      <td>0.658626</td>\n",
       "      <td>risk management and financial institutions.pdf</td>\n",
       "      <td>516</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>liquidity fragmentation</td>\n",
       "      <td>False</td>\n",
       "      <td>0.661040</td>\n",
       "      <td>Market Liquidity Theory evidence Policy Alisa ...</td>\n",
       "      <td>267</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>sharpe ratio</td>\n",
       "      <td>False</td>\n",
       "      <td>0.663047</td>\n",
       "      <td>DesignandDevelopmentofMeanReversionStrategieso...</td>\n",
       "      <td>30</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>market manipulation</td>\n",
       "      <td>False</td>\n",
       "      <td>0.666565</td>\n",
       "      <td>Trading-Volatility.pdf</td>\n",
       "      <td>141</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>delta hedging</td>\n",
       "      <td>False</td>\n",
       "      <td>0.667127</td>\n",
       "      <td>risk management and financial institutions.pdf</td>\n",
       "      <td>200</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>USD collateral tightening</td>\n",
       "      <td>False</td>\n",
       "      <td>0.668989</td>\n",
       "      <td>The xVA Challenge- Counterparty Credit Risk, F...</td>\n",
       "      <td>130</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>spoofing and layering</td>\n",
       "      <td>False</td>\n",
       "      <td>0.670691</td>\n",
       "      <td>gpt deep dive gaps.pdf</td>\n",
       "      <td>91</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>non-stationary time series</td>\n",
       "      <td>False</td>\n",
       "      <td>0.673820</td>\n",
       "      <td>Advances in Financial Machine Learning by LoÃÅp...</td>\n",
       "      <td>103</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>skew arbitrage</td>\n",
       "      <td>False</td>\n",
       "      <td>0.673943</td>\n",
       "      <td>STATISTICAL_CONSEQUENCES_OF_FAT_TAILS_TE.pdf</td>\n",
       "      <td>63</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>mean reversion</td>\n",
       "      <td>False</td>\n",
       "      <td>0.674243</td>\n",
       "      <td>meanreversionins.pdf</td>\n",
       "      <td>35</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>volatility term structure</td>\n",
       "      <td>False</td>\n",
       "      <td>0.678252</td>\n",
       "      <td>risk management and financial institutions.pdf</td>\n",
       "      <td>252</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>market surveillance</td>\n",
       "      <td>False</td>\n",
       "      <td>0.680591</td>\n",
       "      <td>gpt deep dive gaps.pdf</td>\n",
       "      <td>92</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>volume weighted indicators</td>\n",
       "      <td>False</td>\n",
       "      <td>0.681507</td>\n",
       "      <td>Trading algos and systems.pdf</td>\n",
       "      <td>1036</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>portfolio volatility</td>\n",
       "      <td>False</td>\n",
       "      <td>0.681627</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hidden liquidity</td>\n",
       "      <td>False</td>\n",
       "      <td>0.682524</td>\n",
       "      <td>The structure of clearing and settlement -- Da...</td>\n",
       "      <td>191</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>VWAP</td>\n",
       "      <td>False</td>\n",
       "      <td>0.685065</td>\n",
       "      <td>Optimal trading strategies - quantitative appr...</td>\n",
       "      <td>297</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>trading halts</td>\n",
       "      <td>False</td>\n",
       "      <td>0.689157</td>\n",
       "      <td>gpt deep dive gaps.pdf</td>\n",
       "      <td>34</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>fear and greed cycles</td>\n",
       "      <td>False</td>\n",
       "      <td>0.689969</td>\n",
       "      <td>Trading in the Zone - Master the Market with C...</td>\n",
       "      <td>56</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        concept  covered  distance  \\\n",
       "47                 price action    False  0.652456   \n",
       "33    trade execution certainty    False  0.655064   \n",
       "70            scenario analysis    False  0.658626   \n",
       "18      liquidity fragmentation    False  0.661040   \n",
       "59                 sharpe ratio    False  0.663047   \n",
       "182         market manipulation    False  0.666565   \n",
       "159               delta hedging    False  0.667127   \n",
       "130   USD collateral tightening    False  0.668989   \n",
       "181       spoofing and layering    False  0.670691   \n",
       "149  non-stationary time series    False  0.673820   \n",
       "162              skew arbitrage    False  0.673943   \n",
       "49               mean reversion    False  0.674243   \n",
       "169   volatility term structure    False  0.678252   \n",
       "187         market surveillance    False  0.680591   \n",
       "53   volume weighted indicators    False  0.681507   \n",
       "72         portfolio volatility    False  0.681627   \n",
       "10             hidden liquidity    False  0.682524   \n",
       "23                         VWAP    False  0.685065   \n",
       "19                trading halts    False  0.689157   \n",
       "89        fear and greed cycles    False  0.689969   \n",
       "\n",
       "                                                source  page  priority?  \n",
       "47   The Art and Science of Technical Analysis- Mar...    36       True  \n",
       "33                                             Unknown     0      False  \n",
       "70      risk management and financial institutions.pdf   516       True  \n",
       "18   Market Liquidity Theory evidence Policy Alisa ...   267      False  \n",
       "59   DesignandDevelopmentofMeanReversionStrategieso...    30      False  \n",
       "182                             Trading-Volatility.pdf   141      False  \n",
       "159     risk management and financial institutions.pdf   200       True  \n",
       "130  The xVA Challenge- Counterparty Credit Risk, F...   130       True  \n",
       "181                             gpt deep dive gaps.pdf    91       True  \n",
       "149  Advances in Financial Machine Learning by LoÃÅp...   103       True  \n",
       "162       STATISTICAL_CONSEQUENCES_OF_FAT_TAILS_TE.pdf    63      False  \n",
       "49                                meanreversionins.pdf    35      False  \n",
       "169     risk management and financial institutions.pdf   252       True  \n",
       "187                             gpt deep dive gaps.pdf    92       True  \n",
       "53                       Trading algos and systems.pdf  1036      False  \n",
       "72                                             Unknown     0      False  \n",
       "10   The structure of clearing and settlement -- Da...   191       True  \n",
       "23   Optimal trading strategies - quantitative appr...   297       True  \n",
       "19                              gpt deep dive gaps.pdf    34       True  \n",
       "89   Trading in the Zone - Master the Market with C...    56       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Coverage re‚Äëscan (threshold = 0.65) -----------------------------\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd, faiss, numpy as np\n",
    "\n",
    "encoder   = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "threshold = 0.65\n",
    "\n",
    "concepts = pd.read_csv(DATA_DIR / \"concept_list.csv\")[\"concept\"].tolist()\n",
    "\n",
    "coverage  = []\n",
    "for concept in concepts:\n",
    "    q_vec = encoder.encode([concept])\n",
    "\n",
    "    # ---- search priority first\n",
    "    Dp, Ip = priority_index.search(q_vec, 1)\n",
    "    best_dist   = Dp[0][0]\n",
    "    best_row    = meta_priority.iloc[Ip[0][0]]\n",
    "    is_prior    = True\n",
    "\n",
    "    # ---- search main, take if closer\n",
    "    Dm, Im = main_index.search(q_vec, 1)\n",
    "    if Dm[0][0] < best_dist:\n",
    "        best_dist = Dm[0][0]\n",
    "        best_row  = meta_main.iloc[Im[0][0]]\n",
    "        is_prior  = False\n",
    "\n",
    "    coverage.append({\n",
    "        \"concept\"    : concept,\n",
    "        \"covered\"    : best_dist <= threshold,\n",
    "        \"distance\"   : best_dist,\n",
    "        \"source\"     : best_row[\"source\"],\n",
    "        \"page\"       : best_row[\"page\"],\n",
    "        \"priority?\"  : is_prior\n",
    "    })\n",
    "\n",
    "coverage_df = pd.DataFrame(coverage)\n",
    "coverage_df.to_csv(DATA_DIR / \"concept_coverage_0p65.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Scan complete.  Missing / weak concepts:\")\n",
    "display(coverage_df[~coverage_df[\"covered\"]].sort_values(\"distance\").head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2c7002-5fe4-48fd-abda-0230dbb25f63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
